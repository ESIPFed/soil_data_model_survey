[
["index.html", "Soil Data Models: a review 1 Introduction", " Soil Data Models: a review K Todd-Brown (ktoddbrown@ufl.edu) L Heran, K Frederick, M Younger L Nave, N van Gestel, C Scheadel, C Sierra, C Lawrence, A Hoyt, S Stoner, J Beem-Miller, K Heckman, S von Fromm, Á Kuhnen, J Holmquist, W Wieder, S Earl, T Crowther, N Batjes, E Ribeiro 2020 October 19 1 Introduction The overall goal of the project is to develop, implement, and test best practices for compiling transparent, reproducible, harmonized, and extendable data collections for meta-analysis. In this document there is 1) an examination of current meta-analysis efforts in the soil community though short reviews developed in collaboration with the research group, 2) a general interview to the soils community, and 3) a structural summary of the data models available. The results from this project have spawned an upcoming manuscript and future ESIP Cluster [links pending]. This work is based on materials provided by the ESIP Lab with support from the National Aeronautics and Space Administration (NASA), National Oceanic and Atmospheric Administration (NOAA) and the United States Geologic Survey (USGS). "],
["summary-of-interviews.html", "2 Summary of Interviews 2.1 Project motivation 2.2 Workflow 2.3 Decisions 2.4 Attaining copy of project 2.5 Painpoints and suggestions", " 2 Summary of Interviews Our conversations with project teams began in the summer and fall of 2020 and consisted of five main guiding questions covering the general motivation of the project, workflow, decision points, availabilty, and possible improvements. 2.1 Project motivation Specific project motivation to create these larger data sets were varied but generally centered on a global scale queation that could nto be addressed by a single project movitavating the synthesis of multiple data providers. Most participants stated that whatever data they were specifically looking for was unavailable, unorganized, or nonexistent, so they therefore started the project/dataset out of need or frustration to address a specific question. This synthesis process was long; most of the data products developed over a period of five years or longer. 2.2 Workflow All workflows aggregated data somehow and then put it through an automated quality control and assurance process. In general data was prioritized for synthsis based on it’s size (more data was higher priority) and relevancy to the question of the project. Much of the diversity in these workflow came from the aggregation process and the degree to which manual data transcription was utlized. The most common approach entailed manual data transcription from the orginal format (data table or information enbedded in a figure) to a study specific data template. This data transcription was done in some cases primarily by the data provider (ISCN, ISRaD, WoSIS, CC-RCN, and Crowther), or by the data synthesis team themselves (ISRaD, van Gestel, and SIDb). SoDaH had a unique key approach where a data thesaurus was developed by the data provider to translate the orginal data model into the common project model. In all but one case (SIDb) the data model consisted of a tabular relational database. SIDb in contrast had a unique tree structure with embedded rectagular data. 2.3 Decisions Workflows were a balance between ease of use and robustness. Very large (ISRaD) and very small (van Gestel) team projects tended to convert to a data transcription template approach for ease of use and short development time. Intermedate projects like SoDaH had the motivation to produce a more robust pipeline and the nibleness to experiment a little more. Across all projects communication, both within the synthesis team and between the synthesis team and data providers, was critical. Working ‘hackathons’ were identified by ISRaD, SIDb, and SoDaH as being critical for moving the project forward. Documentation of the template was critical for teams of more then a single PI. 2.4 Attaining copy of project Notably online repositories were not the primary access point for synthesis products. Data was generally stored in a GitHub repository or hosted on project websites. The data was mainly formatted as relational data tables, frequently through Excel. 2.5 Painpoints and suggestions Common pain points stated by the principal investigators were the generational transfer of the data product, the required skill mix of informatics and soils, and the complex complete data structures which lead to large amounts of unique vocabulary. Data product systhesis efforts typically take longer then a standard 3-year funding cycle. Thus data products are often repurposed to addresss slightly different questions over the course of their lifecycle (ISRaD is a particuarlly good exampel of this). Both the inhirent complexity of soils data as well as this repurposing frequently led to changes in the project data template. These changes in template structure were frequenctly challenging to impliment, requiring revisiting of the orginal data source to ensure a complete capture of the data with the new template. Often these soil data templates ended up being particularly large and diverse, consisting of 100’s of unique columns (ISCN, ISRaD) unless deliberately avoided (Crowther, WoSIS). Finally teams frequently identified the unique skill combination of soils science paired with computure programming or database expertise, as being particuarlly difficult to find. To combat some of these concerns, the researchers suggested ensuring to start with the correct team, involving both soil scientists and scientists with informatics backgrounds, as well as taking more time to develop the template before its use, so it would not need to be modified later. Positive suggestions included, having the community weigh in on needs and wants for the database and hosting hackathons and workshops to focus on completing tasks. This ensures that the final data product will be completed efficiently and that when completed it will be used. "],
["iscn.html", "3 ISCN 3.1 June 2020 interview 3.2 ISCN3 and template data model 3.3 Acknowledgements", " 3 ISCN 3.1 June 2020 interview These are notes from a 4 June 2020 interview with Luke Nave. Why did you start this study/project? The study was motivated by the desire to draw together scattered data into a common place with a common format so that users could apply it to answer outstanding questions in the carbon cycle. These scientific questions at the time, 10 years ago, included: How much soil carbon is there and where is it? Given that we have stocks of X, how are these stocks changing with land use change, climate change, and management? There were also operational/methodical questions to be answered: Can improved coverage of carbon stock data result in better model representation of where carbon soil is? Can data on soil fractions and turn over times improve process representation in models? Describe your workflow for ingesting data sets? Researchers identified which variables in the desired source dataset cross-walked with which variables in the ISCN template as well as identified where new variables needed to be added. Data providers then filled out an Excel template that was then ingested into the ISCN database using SQL commands. The greatest efforts were focused on the largest datasets. The threshold for adding a new variable to the template was high ~90% of the final variables were in the original. Tried to be expansive then so they would not have to add more variables later in order to incorporate relatively common variables However, adding new variables was on a case to case basis. For example, carbon and bulk density variables were more likely to be added with new methods, versus cation exchange capacity because carbon stock variables were what the dataset focused on. The templates evolved with each version but changed less than the data product. Started with &lt; 40,000 profiles and ending with &gt; 71,000 The documentation got better with each version. What decisions did you make to arrive at this workflow? Combined two existing data structures from the flux community (Ameriflux) and the soil community (NRCS), because these were the two largest contributors to the data infrastructure. There was extensive work (a series of workshops, beginning with an Alaska-focused project) with the soil, carbon cycle, and biochemistry research communities to make sure that the structure worked for single investigator driven soil datasets. How would someone get a copy of the data in this study? Go to the ISCN website and download a static excel file. There is a quick-start, intermediate, and advanced user guide. What would you do differently if you had to start again? Develop products in parallel with the database itself. The interviewee’s great disappointment was overemphasizing consolidating the data for their own sake but not spending enough time turning the data into publications. This could have shown what can be done with the synthesis to spark interests and show what is possible. Have a small group of people (5 or 6) executing a common set of standard gap fills and decision trees for bulk density, carbon concentrations, and stock computations. They would set rules and create commonly agreed upon couple of scripts that could do some gap-filling. This would save time for future users by just getting that done first. ISCN3 has complex gap-filling methods that we used to generate soil organic carbon stock estimates and flagged this under a relatively cryptic headers; these were the product of only 2 people and much of the decision making is only documented in a mess of old emails. Put all the code that handles the data that turns it from data into product in a common format and allow people who are interested to get it and edit it. This would make it open access which was not a part of ISCN (the Scientific Steering Group specifically desired to control and document access, in order that data contributors received credit and citations). The current codebase used to generate ISCN3 is cryptic to the point of being black boxed after the retirement of the original programmer. Create a functional way to harvest information about treatments and disturbances, so that the dataset can be used to study treatment effects. The disturbance table was prototyped but never satisfactorily implemented, much less refined through implementation. Change latitude and longitude to profile level rather than site level attributes to create high resolution data. Have better documentation about how to cite your data contributors and how to make sense of the data products. What would be the same? The capability of incorporating a lot of different kinds of data. The templates were generally adequate for most data needs we encountered. In terms of interfacing with the broader science community, listening and having workshops were helpful in that they resulted in positive changes to the templates. Allow for map based data retrieval to increase usability by allowing the user to filter by place and by variables. The user would no longer have to deal with large excel tables where most of the columns are empty anyways. 3.2 ISCN3 and template data model Common features location, elevation, and observation time depth of sample and layer bulk density vegetation carbon percentage and loss on ignition Unique features disturbance table high level of site details frost free days, ponding, runoff higher then average number of layer-level info fraction table only shared with ISRaD 3.2.0.1 ISCN3 3.2.0.2 ISCN-Template 3.3 Acknowledgements Special thanks to Dr Luke Nave (University of Michigan) for his help with the interpertation of ISCN. "],
["van-gestel-record.html", "4 van Gestel record 4.1 June 2020 Interview 4.2 van Gestel data model 4.3 Acknowledgements", " 4 van Gestel record 4.1 June 2020 Interview These are notes from the June 2020 interview with Dr Natasja van Gestel. Why did you start this study? There was a DOE funded project during van Gestel’s postdoc where it became obvious that the modeling project would benefit from a more robust data set for parameterization. This became an ongoing project over the past 5 years. Describe your workflow for ingesting data sets? Data is extracted from the published literature, generally though transcription of points recovered from figures or published tables. Studies are selected based on availability of soil carbon, and an attempt is made to capture all the data associated with these studies. This data is then entered into an Excel spreadsheet, fed through a series of scripts to automate gap-filling, and then generate data products relevant to modeling studies. van Gestel and her lab were responsible for everything from the initial design of the spreadsheets, to data recovery and post-processing. There are currently over 100 studies in the database. What decisions did you make to arrive at this workflow? In general the design of the spreadsheet has not changed significantly over the years. To design the spreadsheet van Gestel tried to think mechanistically about the processes that control carbon cycling in soils and create an exhaustive list of variables to capture from the literature. This exhaustive template was then pruned to create more restricted data products as needed. Expanded on factors originally related mostly to carbon as represented by soil carbon models (ie carbon stocks, fluxes, experimental treatments for temperature, nutrients, and moisture). Simplicity was a core value in developing the data model. Tried to preserve as much information from the origial data source as possible. Needed to record original units and then harmonize units in processing scripts. Key to determine how bulk density was treated in the study Sometimes there were no bulk density values, some studies only used 1 value for bulk density or a prescribed standard bulk density. Bulk density was frequently gap-filled in the generated data products using imputed bulk density to organic carbon relationships How would someone get a copy of the data in this study? Contact van Gestel directly, publication is pending. What would you do differently if you had to start again? What would be the same? Happy with the outcome of the meta analysis and would not change much. Current template and data product pipeline has proven effective. Currently using powerpoint to extract data from figures, so using a different type of software could make the process more efficient. Considering focusing more on collecting data from repositories in the future. 4.2 van Gestel data model Common features location, elevation, and observation time pH bulk density Unique features mean depth instead of depth of core and layer carbon, nitrogen, and phosphorus pools above and below ground treatments and information about treatments (mean, standard error, size) ‘input’ variable soil horizon and percent soil organic matter 4.2.1 Current version 4.3 Acknowledgements Special thanks to Dr Natasja van Gestel (Texas Tech University) for making the metadata for this data product available for analysis and making herself available for interview. "],
["sidb-record.html", "5 SIDb record 5.1 June 2020 interview 5.2 SIDb data model 5.3 Acknowledgements", " 5 SIDb record 5.1 June 2020 interview These are notes from a 4 June 2020 interview with Carlos Sierra and Christina Scheadel. Why did you start this study? There is this huge potential to use new innovations with soil incubation data, but people were using it in many different ways, making it hard to demonstrate it’s value. Time series incubations are an important aspect of soil incubation that have been under utilized, in contrast to total respired fraction. In particular time series incubation data can tell you about the carbon stock rate and the dynamics. The fact that the environment in these incubations is so tightly contolled is a feature, not a bug, removing confounding variables (in other words, sensitivity functions) from the results. This database is a merger of two projects by Carlos and Christina. These two projects were started roughly 5 years ago; once they both became aware of the other’s projects, they merged efforts. A week-long workshop in Jena, Germany during March of 2019 accelerated work. Describe your workflow for ingesting data sets. There are three main elements of a data contribution: the time series, initial conditions, and metadata. The templates have evolved over time, especially in the March-2019 workshop. A complete template is essential to enable consistent information capture from the original manuscripts. Time series are captured from graphs in original manuscripts, and occasionally direct data from the original PIs. Data was generally not directly contributed from PI unless the PI was directly involved in the data product. SIDb is NOT a repository and instead a dynamic data product. Data should ideally be archived in a repository seperately. Follow up question: How successful were you in getting PIs to give you data? Within the group, there were people with data, so they got data from them instead of contacting PIs for them. Did not ask for data from PIs. the dataset is not complete at all. They were setting up the infrastructure for people to upload their own data. No outside contributions yet. Some people have expressed interest. Dynamic database, not a repository. Contribution to a product. You are expected to archive your own data. Current data product There are 31 studies so far. 684 total time-series The longest time series is 800 to 1,000 days. What decisions did you make to arrive at this workflow? Driven by research questions, design is mostly trial and error. Reevaluate structure and workflow as they worked, or as needed. Going forward, when incorporating additional columns, their thoughts are that it needs to evolve based on the needs of that specific entry. How would someone get a copy of the data in this study? git clone a GitHub repository and then interact with it through the associated R-package. R package reads the data, use simple plotting, and model fitting via the FME R-package that can use both classical gradient and MCMC methods. What would you do differently if you had to start again? What would be the same? Having the project be reserach question driven worked well even if it was a bit slow. Would have been good to collaborate sooner. That might have moved the project along faster. Excellent group dynamics “If you have a group that you feel comfortable with and where everyone is well-coordinated, ideas will flow much better in a good direction.” Christina said the group talked every other week and that everyone was very responsive, making the group great. Common research goals united the entire group. Small workshop worked exceptionally well (less than 5 people) More or less a goal in mind than a structure. Goal: Have a paper somewhat completed Had a whiteboard There is a lot of disconnect between the informatics researchers and domain scientists. It feels like there should be more cross talk between the two then there currently is. 5.2 SIDb data model Common features location, elevation, observation time depth vegetation Unique features anaerobic/aerobic gasMeasured samplePreparation more detailed for permafrost initial conditions time series experimental information is captured in the column names of the time series The soil incubation data base has a unique tree structure instead of a relational database that, in it’s native form, is represented as follows. The data base is a list of studies. Each study has a single root with the following struction. For the sake of comparison we deconstructed this tree structure and describe the SIDb data product downloaded June-2020 with the following set of relational data tables. This is currently pending work. 5.3 Acknowledgements Special thanks to Drs Christina Scheadel (Northern Arizona University) and Carlos Sierra (Max Planck Institute for Biogeochemistry) for making the meta data for making themselves available for interview. "],
["israd.html", "6 ISRaD 6.1 June 2020 interview 6.2 ISRaD data model 6.3 Acknowledgements", " 6 ISRaD 6.1 June 2020 interview Why did you start this study? ISRaD started prior to 2015 as an informal discussion at the AGU Fall Meeting to address several questions around controlling factors of soil carbon stocks. This lead to a USGS, Powell Center proposal, focusing particularly on the role of mineralogy and fractionation to control soil carbon stocks. The original workshop intended to explore controls of carbon storage and persistence in soils. Part of the group had a more specific intent of looking at various proxies for mineral or metal stabilized carbon in soils. In the course of these discussions, radiocarbon and other isotope measurements were recognized as being important to address questions of timescales. These mineral and isotope focused questions guided the initial template development. The ISCN template was extended and modified with this in mind. The ISCN template required extentions to address missing fractionation schemes (template was entirely focused on bulk data) sparse mineralogy and exchangeable metal data. overly complicated factor fields describing land use and other variables overall extremely complicated and difficult to fill out template structure To address the above concerns, the Powell Center group spent considerable time revising and extending the ISCN template to include fraction information, simplifying land use description schemes, and generally trying to streamline the template. Some data sets were ingested using the template, mostly as test cases for template development. The Max Planck Institute for Biogeochemistry picked up the project in 2018, providing support for larger data ingestion efforts primarily driven by a desire to compare Earth system models to soil radiocarbon data. Describe your workflow for ingesting data sets? Enter data into the ISRaD template from (https://soilradiocarbon.org/contribute/)[https://soilradiocarbon.org/contribute/] Either done by data provider (with assistant from MPI team) Or, if identified as a high value dataset, MPI teams will fill out template. Once the data is entered into the template, it can be QAQC-ed using scripts available from the R-package or through an online GUI. This rscript uses a combination of controlled vocabulary, string matching, limits with quantitative data, and ensuring that the names and the keys match to check the submission. The data and output from the QAQC script is then sent to the ISRaD data coordinator via contact. The data coordinator then forwards the data for expert review to one of the 6 or 7 reviewers who make recommendations and changes. It then goes back and forth a number of times between the data submitter and reviewer. Once the data is vetted, it is ingested into the ISRaD data product. What decisions did you make to arrive at this workflow? ISRaD wanted to develop a simpler data template within an open science framework. The revised template proved to still be challenging to fill out necessitating the review process. How would someone get a copy of the data in this study? A person who wanted the data could go to the ISRaD website (soilradiocarbon.org) and go to the database tab where the data is available for download It is also available directly from the package The website is hosted off of the github repository What would you do differently if you had to start again? Data model iteration and development is necessary but extremely challenging, especially as the data product grows Data templates are complicated to fill out, requiring time and training. You need both domain and data science skills. Make sure to start with the right team. Original project had soil scientists but did not have people who had strength in coding or database structure. There is a clear need for soil scientists with database training. Try to think more in advance about the template and its structure Template was changed a lot while entering studies so they had to keep going back to old studies that they had already finished to find more information Actively identify the 15 studies that they cared most about and be more methodical Have a structure to work with ahead of time vs building the thing around the studies The soil science community needs to establish best practices background information for published students. Studies are occationally missing basic information like lat-lon location of sites. Unclear where the next home for the data product is as the current team moves on to their next phase in their careers. What would be the same? One of the strongest aspects of ISRaD is the combination of automated QAQC and expert review Another is the transparency of the dataset in that any data that is in the template is very traceable about where it came from and how it got there Every data point is documented. Every study that is entered has to have a DOI. Continue to promote open science. ISRaD is an open source and is available on GitHub which allows for it to have version control. The data product had really good scientific output. It got a lot of different groups to use it and answer different questions, which has helped push it forward and keep it relevant by bringing in other groups Hackathons worked really well. Get everyone together for a few focused days for dedicated push sessions to get a lot done. It works because everyone commits to a full couple of days and can ask questions and not have to pause to contact someone. 6.2 ISRaD data model Common features location, elevation, and observation time depth of core and layer bulk density vegetation carbon percentage and loss on ignition Unique features disturbance table high level of site details frost free days, ponding, runoff higher then average number of layer-level info fraction table only shared with ISRaD 6.3 Acknowledgements We would like to talk Drs Corey Lawerence (USGS), Kate Heckman (US Forest Service), and Alison Hoyt (MPI) as well as Shane Stoner (MPI), Jeffrey Beem-Miller (MPI), Sophie von Fromm (MPI), Ágatha Della Rosa Kuhnen (MPI) for their time and contributions to the June interview. "],
["cc-rcn.html", "7 CC-RCN 7.1 June 2020 interview 7.2 CC-RCN data model 7.3 Acknowledgements", " 7 CC-RCN 7.1 June 2020 interview Why did you start this study? On a community level, coastal wetlands are a really big but a really unknown part of the carbon cycle and there is a lot of potential for coastal wetland management to contribute locally and nationally to offsetting greenhouse gas emissions. There is a lot of policy interest in coastal wetland carbon. This started with the 2013 IPCC wetlands supplement where they called for nations to account for coastal wetlands. They put together some first cut estimates of what average emissions are based on pulling values out of papers, but it was not super satisfying to the community just to have those literature values when there is known to be much variability in soils. Everyone realized that there was a need to do a better job at data synthesis. This came to a head 2 years before the project started (4 or 5 years ago, ~2015). The North American Carbon Program hosted a meeting and invited prominent scientists, managers, and people who had interest in blue carbon and asked “what do you want out of blue carbon synthesis?”. J Holmquist was at the meeting because he had been part of a NASA project with a lot of the synthesis of raw data and gave a speech about what he had learned. This sparked a lot of discussions about who would host the database. Started learning about the small politics of data: how do we share data? Who owns it? and tried to facilitate a data sharing program where everyone wins. Smithsonian Insitute was a logical choice to host the database because it is widely trusted in the community with experience archiving data. On a personal level, Holmquist really wanted to drive this forward because he had such a hard time putting the NASA synthesis together, but he was also able to answer some really interesting high level questions with it. He felt that a lot of other people could use the dataset to answer more and better questions and wanted to allow for this access. He wanted more people to do the work that he was doing without having to go through the issues of calling everyone in the field. Describe your workflow for ingesting data sets? In order to get datasets to ingest, there were a lot of interviews with data providers and one on one phone calls. Emails are easy to ignore and the contact allows for trust to be built. Just getting people to send you data takes so long, partially because people are busy or they are worried about issues with the data. They can be sensitive or embarrassed at how their data looks. Project management wise, there is a Google Sheet that ranks the datasets and to-dos on priority, based on how easy and important it is to do Top shelf cores with good age depth models go straight to the top of the list There is also a column for when data was submitted. It is only when data is actually submitted that it gets put to the top of the list behind the data that was submitted before it. The overall process is first going through the interview stage or reading the paper, then coding out the methods metadata, then doing an initial data inspection by going through 1 or 2 drafts of a data release, and finally uploading it to figshare. Custom R functions and hook scripts are used for preparing the actual data releases. They do things like take quick looks at the range of the datasets to make sure that everything is how it should be, such as fractional units are not above 1 or below 0. They use a lot of tidyverse packages to help keeps things organized and readable. Code organization is important because multiple people work on a script When preparing data releases, they prepare EML style metadata and a package called EML down which creates a nice looking html page version When going back and forth with data submitters at the last stage, a data visualization report is created. It is a markdown document with a lot of plots such as bulk density vs loss on emission + Allows for outliers or mistakes to be spotted + Helps data submitters find mistakes in the data at this point Primarily done by the equivalent of one full time technician. Data ingest is a lot of work. What decisions did you make to arrive at this workflow? Lessons learned from previous experience constructing a single PI meta-analysis and experience in management led to the current pipeline for CC-RCN. How would someone get a copy of the data in this study? GitHub repositories For every study ID, there are one or more citation files that get downloaded as bibtex files to ensure easy citation. What would you do differently if you had to start again? What has not worked: trying to teach other people how to use github. The barrier to doing more is limited person hours to devote to creating this community. Do not know if it is worth it to do so, because it is unknown how much can be gotten out of it Want to try creating partnership with grad students. They can trade resources such as helping give a boost to a dissertation in return for helping build the database. This would take time and attention to plan There is not a good model to look for for community built in. Wanted to try to make this a bit more of a community effort and more open source but have not found a good way to do it. Technical issue is that some of the entries in the database are synthesis of synthesis Capturing complete methods is challenging due to lack of time to go in and fill in all the methods Better pipeline for unpublished data; right now there is no way to include it in the synthesis. This is sort of by design since it does not align with values of the project. Not everyone is educated on data licenses What would be the same? Come up with a letter or a memo of data responsibility that describes the project, states where the data is going to be kept, and creates working definitions of the data. It makes it so your data is your data until you say it is public. Talking to people on the phone is more efficient than emailing Understand the small politics of the data and be sensitive with the people who are trying to participate but are struggling Host one data carpentry event a year or more In order to participate in the synthesis, one has to learn certain tools and work practices Try as much as possible to use github Different github repo for every sub-project (the dataset is in one github repository, the app for visualizing is in another, and different working groups have their own) Use gitignore files to make sure not to accidentally upload any private files Have a value statement instead of rules because one comes across things that are unexpected. Value statement is more adaptable to these new situations but keeps integrity Start off with having a community weighing in and saying what they need and want from a database. Release a couple different versions of original guidance to receive commentary and feedback A lot of the success is due to the fact that the community was small enough where everyone knows each other, is willing to participate, and is easy to reach, but also big enough to where there were a lot of people who need to work together and are. 7.2 CC-RCN data model Common features location, elevation, and observation time depth of core and layer bulk density vegetation carbon (?organic) percentage and loss on ignition Unique features min/max latitude detailed author information ‘one_liner’ summary break out bulk density mass/volume many specific isotopes listed (Am241, C14, Cs137, Be7, Pb210, Ra226) X_class is free text or control vocabulary coastal specific vocabulary inundation/salinity anthropogenic impacts core-level vs site latitude/longitude and elevation 7.3 Acknowledgements We would like to thank Dr James Holmquist (Smithsonian Institution) for his time and contributions to the June interview. "],
["crowther.html", "8 Crowther 8.1 July 2020 interview 8.2 Crowther data model 8.3 Acknowledgements", " 8 Crowther 8.1 July 2020 interview Why did you start this study? The more data you collect the more patterns you can identify against the noise. Global data is needed to identify global patterns. This project specifically worked to address soil carbon sensitivity to warming. But in general, the Crowther lab is interested in the living components of the soil, both in categorizing and characterizing these living components. For example, if at two locations with the same climate, topography, and vegetation but different sets of soil organisms the functioning and carbon turn over would be very different. Describe your workflow for ingesting data sets? The data template has only asked for a brief description of the site location (latitude, longitude, ect.), the measurement they are looking for their project, and how the author collected that measurement. This very restricted data model is then much easier (and more satisfying) to collaborate with data products to fill out. Next, data producers are contacted directly and invited to co-author the resulting manuscript. The resulting manuscript generally has between 50 and 100 co-authors and averages 5,000 to 10,000 samples or locations across the globe. This simplistic approach is far more productive than having large tables that need computation and large amounts of time to fill. What decisions did you make to arrive at this workflow? For the inital soil warming study, we wanted to quantify differences in carbon stocks in warmed and ambient plots. Thus %C and bulk density estimates were required, but we expected that the duration and intensity of warming are also likely to have strong effects so we collected that data to try to control for those effects. In addition, we knew that environmental variation would be likely to affect the magnitude of the changes, but this information was difficult to collect from all projects in a standardized way. We were trying to collect all the data possible and would end up with a table of 50 columns that are mostly incomplete. This drove us to infer possible driving variables from global data map products. In the end, we found that the data is much stronger when the missing data is taken from existing maps where experts have done correct extrapolations. They can then get all the metadata necessary just from knowing the latitude and longitude of the sample. We got better results with a small, easy to fill out data request from the data providers then a large verbose data template in the end. This allowed us to maximize the data volume that we were working with which was a high priority for us. To get the environmental characteristics at each sampling location, we use data from existing maps of climate, soil and topographic characteristics. Those have some uncertainty. But it ensures that we get a complete dataset, and we limit the uncertainty associated with sampling variation between people. How would someone get a copy of the data in this study? After they are published, they are made available in their university library. What would you do differently if you had to start again? What would be the same? Asking for small targeted data has worked very well. Managing the large co-author list and people skills remains challenging. Tom spends so much time on managing emails, or dealing with failed communication attempts due to switching of emails or being sent to spam. He believes that managing relationships with the authors of the data is the hardest, because if you miss one person, you could be missing a large opportunity for your project. Recently, he has been using listservs as he finds it reaches more people, is less likely to be filtered out of people’s professional emails, and is easier to keep organized. 8.2 Crowther data model Common features location, elevation, and observation time depth of core and layer bulk density vegetation carbon (?organic) percentage and loss on ignition Unique features Author updated data (outside sources) Biome, % Clay, pH, Detailed soil warming data planned temperatures, control temperatures, mean temperatures Cation exchange capacity reported % Nitrogen reported distinguished between total raw carbon and total carbon Difference between detailed_site_id(New Name) and site_id(Old Name) 8.3 Acknowledgements We would like to thank Dr. Tom Crowther (ETH Zürich) for their time and contributions to the June interview. "],
["sodah.html", "9 SoDaH 9.1 July 2020 interview 9.2 Data model 9.3 Acknowledgements", " 9 SoDaH 9.1 July 2020 interview Why did you start this study? This study grew out of frustrations with the organization of the Long Term Ecological Research (LTER) data and expanded to include Critical Zone Observatory (CZO) and National Ecological Observation Network (NEON). They found that understanding the history of the data and matching dataset variables up with one another was nearly impossible given the published information. To solve this, the study brought the researchers familiar with these data sets together with data aggregators. Describe your workflow for ingesting data sets? In person meetings were critical. 22 person workshops were needed to get the right people in the room with their data in order to learn about the datasets and structure. They used a “key-key” template to harmonize data, where data providers manually map their data set onto a database template (modeled after ISCN via ISRAD) and provide additional site information and metadata. These key-key files are used to rename variables &amp; convert units from raw data into harmonized sheets that are concatenated together in a scripted workflow. On the SoDaH GitHub https://lter.github.io/som-website/workflow.html, the workflow is listed as follows: Harmonizes raw data into a common format using key-key approach in R; Aggregates data into an R object; Data visualization and interpretation tools (Shiny app); Data analysis and summary scripts. The overall project timeline was not entirely smooth. First data was contributed by 22 people during the first meeting, then within the next nine months everything was harmonized using scripts. A second meeting was required to address unanticipated template shortfalls. For example, identifying time series and experimental sites, what the treatments were, and how to identify controls. The data mostly consists of basic soil data with very little fractionation and radiocarbon data that was considered ISRaD provenance. The template had to be expanded to include time series and experimental data. What decisions did you make to arrive at this workflow? Lead PI had slow and painful experience with manual data transcriptions in previous project. The group then sat in the workshop room for about two days before it began brainstorming a plan. After looking at the new ISCN data ingest plan, they decided to attempt a key-key file. A key-key file would map the column names to some common vocabulary and give basic meta data like units. Data providers would add their data into a Google folder with the key-key file was held. An R script would then process the raw data and templates producing a flat-file for each dataset of consistent format. This spur of the moment plan worked surprisingly well. The biggest challenge was getting people on the same page when filling out the key-key file. Many people would add in data that was not asked for or needed. Post processing manual QA/QC was still required and very time-consuming. The site-level info was in one key-key file which held variables such as climate data and site characteristics, and layer-level info was held in another file. The key-key file can be seen on the SoDaH GitHub under the database tab. By sticking all the level one data into flat .csv files, they only manipulated units to match with very simple math. They then had to calculate or recalculate things like nitrogen carbon ratios (if they were not provided) to knit the data into level 2. How would someone get a copy of the data in this study? https://lter.github.io/som-website/index.html Project website with data GUI and workflow outline Wieder, W.R., D. Pierson, S.R. Earl, K. Lajtha, S. Baer, F. Ballantyne, A.A. Berhe, S. Billings, L.M. Brigham, S.S. Chacon, J. Fraterrigo, S.D. Frey, K. Georgiou, M. de Graaff, A.S. Grandy, M.D. Hartman, S.E. Hobbie, C. Johnson, J. Kaye, E. Snowman, M.E. Litvak, M.C. Mack, A. Malhotra, J.A.M. Moore, K. Nadelhoffer, C. Rasmussen, W.L. Silver, B.N. Sulman, X. Walker, and S. Weintraub. 2020. SOils DAta Harmonization database (SoDaH): an open-source synthesis of soil data from research networks ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/9733f6b6d2ffd12bf126dc36a763e0b4 (Accessed 2020-07-16). Manuscript to ESSD submitted (July 2020) What would you do differently if you had to start again? What would be the same? Orginal key-key template turned out to not be sufficent to harmonize the data and required revision. It would have been a good idea to beta-test the template before they had people in front of them at the workshop. This also affected the derivative data model; that would have been better with a cleaner key file. It would be nice to link the key-key file with the data repository metadata to automate portions of the creation of the key-key file. Having a dedicated data manager was extremely beneficial. It was sometimes hard to get data science and soil science on the same page. After having gone through this process, the PI can see the value and need for a control vocabulary and definitions and wishes they had gone through this process before the project began. Something that went well was the semi-automated sequence for harmonization. Compared to past expereinces with manual transcription, it was way more productive and allowed for a faster timeline with fewer people-hours. None of the data providers were nervous to share their data; that wasn’t an issue at all. Some people came to get their data published for the first time. They did struggle a little bit because the data collection wasn’t focused, just all soil data. They did not exactly know what their research question was or what they wanted to do with it. They believe that maybe not having that focus was a detriment. Additional struggles included finding network data all in one place and not having a strong research question, which made explanation, validation, and funding of the project harder. To reemphasize, Will stated that originally they only had the money for two meetings. However, they were able to stretch it into three. The group wanted to achieve something similar to the Israd dataset. Will wanted to interpret and organize all this data into a product first, and then they could all brainstorm the way in which they would want to use it. Overall, the money ran out quickly so it has moved on to mostly volunteer work due to the fact that synthesis takes so long. Will spoke about how COVID is making things harder, but eventually he would like to hold another meeting for the project. 9.2 Data model The SoDaH data product has a single data table with 165 columns. The SoDaH data key-key has 5 data tables (location, profile, layer, and fraction) with between 5 and 108 columns. 9.3 Acknowledgements Special thanks to Dr Will Wieder and Steve Earl for their time to conduct this interview. "],
["wosis.html", "10 WoSIS 10.1 September 2020 interview 10.2 Data model 10.3 Acknowledgements", " 10 WoSIS 10.1 September 2020 interview What is the history of ISRIC and WoSIS as a database? ISRIC (International Soil Reference and Information Center), presently known as ISRIC - World Soil Information ISRIC is an international soil center that was founded in 1966 following the UNESCO council This council showed the need for an organization that could give examples of typical soil profiles ISRIC started as an international soil museum, but eventually moved digital to databases Since 1989 ISRIC has been accredited as world data center for soils by the international science council It provides standardized data for soils of the world, both global and continental soils WoSIS (World Soil Information Service) While ISRIC is the organization, WOSIS is the database Created around 5 or 6 years ago by compiling databases that ISRIC already had Many of the initial databases were data compilations so there were a lot of replicates that needed to be condensed Harmonized World Soil database is a different product from WoSIS, evolved from FAO-UNESCO Soil Map of the World Combines existing regional and national soil information FAO classification drove decision rules SoilGrids Uses soil profile observations from WoSIS and a stack of environmental co-variates to map the spatial distribution of soil properties Describe your workflow for ingesting data sets? First, look for datasets that contain soil data or people contact us with datasets Next, get a license to use the data Finding people/organizations willing to share their data can be difficult After these steps, we already have data as it arrives to us and a license, so we then generate metadata We have a server The metadata follows the European standards Questions and parameters always the same no matter the standard In the past, we would process the datasets one by one Recently, we have run it in a bunch, processing 18 datasets last year and 30 this year Have a script (python) that reads all the source formats Then separate the formats into different folders Turns sheets into tables Now we have 500 tables which are not really related but you can kind of see what matches up Another schema that consists of datasets, tables, and columns Start assigning the columns to our ontologies Results in a table that has a list of attributes and units and methods Another script puts everything together Matching the tables that have related variables Have two tables, one that describes the sites and one that describes the layer and descriptions In general, we expect the data to be a certain structure, but not all follow this structure so expert curated manual conversion is needed What decisions did you make to arrive at this workflow? There have been a number of versions of WOSIS As you progress you see some things work well and some don’t so adjustments are made Inside ISRIC, they did not have one compiled database but there was a need for one that was flexible enough to incorporate all the datasets they had This is how WoSIS came about. Subsequently, data sets shared by the international community were added to WoSIS for standardisation Another project started to make derived soil property maps (SoilGrids250m) for the world using digital soil mapping. Found that they needed more data to do this, but there was already a huge (30) stack in pipeline Currently processing them in batches. Bottleneck remains sharing (CC BY) of soil profile data for future use by the international community How would someone get a copy of the data in this study? Go on data.isric.org to access the Isric Data Hub, search for ‘wosis’ Searchable according to keywords or types of databases Able to download data Have a central database that they work on (WoSIS) and then extract ‘shared data’ that has been standardized and validated Snapshots 2016 and 2019 are frozen from that time Allow other people to add metadata if they want to be included in the registry Consist of a doi for consistent citation purposes and metadata This data is static In a TSV format that can be downloaded as zip files WOSIS_latest contains the same variables as Snapshots Consists of the more recent standardized data from WoSIS It is a dynamic set due to correcting errors if the occur and adding in new data What would you do differently if you had to start again? What would be the same? Avoid data compilations It brings ‘trouble’ when one starts to compile compilations because it creates overlap and repeated data It is difficult to find which data are repeated Becomes very cumbersome to clean it Instead of using compilations, use source data Conversely, if you are using a compilation, make sure to not also include a reference to the source data Do things in batches Might not have as much time to go through details, but the time saved is worth it Try to steer away from excel people put things with colors, get bad habits Does not work as well with huge databases where it is difficult to see everything on huge tables Don’t worry about quantity, worry about quality of databases Stick with open standards and open formats Open standards help with giving the project a future Open formats make it more likely that people will use it 10.2 Data model 10.3 Acknowledgements Thank you to Drs Niels H. Batjes (ISRIC) and Eloi Ribeiro (ISRIC) for their time to talk about WoSIS and other ISRIC activities. "],
["community-survey.html", "11 Community Survey 11.1 Summary of responses 11.2 Responses", " 11 Community Survey Many of our respondence were looking at questions around global biogeochemical processes, specifically carbon cycling. Most of the data sources were drawn from the primary scientific literature with several respondences also drawing from direct response from study PI or colleagues. One respondent mentioned online databases (e.g., USGS NWIS, USGS Geochemical Landscapes, NEON). None of the study respondences mentioned DataOne or PI-driven data repositories for data discovery. Identified challenges included QAQC protocols, difficulty getting data upon direct request from collegues, formatting heterogenity, missing or badly identified metadata, template development and stability, project management for aggregation effort in large groups. Things that worked well in the process were orchestrated hackatons. In general many respondants want data paired with a publication and already entired into a universal template. There was frustration with brittle inhouse harmonization pipelines that were not always backwards compatable. Automatic discovery from primary literature made another wishlist. General FAIR data principles were recognized as important, though not named as such. In general there were no differences in the harmonizatin pipeline process between the providers and aggregators. Unsurprisingly open public data was well supported by the respondences with interest in broaderening scienific impact and increased visibility. Attribution and engagement was a named concern of respondences, in particular collonial science was called out as being partiucalrlly problematic. Time investments with unclear rewards. Unclear what “data base” was appropreate for a given data set. Data permissions issues. Data is messy and data providers wish that was better understood by data aggregators. Both error/uncertainty and the exact methods use things that data providers feel is often overlooked by data aggregators. 11.1 Summary of responses The survey followed the interviews and had a total of 23 responses. Even though the levels of experience and purposes behind compiling data products varied, there were many commonalities across all respondents. 11.1.1 Finding sources When asked how they find their data sources, the majority of participants indicated that they used literature, while others used databases, reached out to colleagues that had worked on the topic, or even used a combination of all three. These responses indicate that data accessibility is much less of a problem now than previously with conducting a meta-analysis. 11.1.2 Painpoints and suggestions When asked what went well with compiling the data product or what could have gone better, most people listed difficulties rather than successes. Of the few successes, people reported comprehensive search results and the standardization of metadata and control vocabulary. The difficulties of compiling the data product included trying to determine what authors meant when there was a lack of documentation, and the large amounts of time spent to sort data and create a template. One respondent concluded that it was easy to automate ingestion of data from the same source, but very time consuming to mix data formats due to the need for harmonization. 11.1.3 Ideal process The participants shared that the ideal process for data harmonization involves gap-filling, standardizing units and table structure, as well as being able to preserve the original data yet build off of it for their own purposes. 11.1.4 Main hurdles Lastly, the main hurdles to contributing to data products and to other people using data include the time sunk into curating and formatting data products, the complex organization of data, and finding which data products to contribute to. Overall, the difficulty of meta-analysis results from trying to harmonize such complex and diverse data sets. 11.2 Responses 11.2.1 Why do you compile data product(s)? What is the question you are looking to answer? ## To understand global variability and predictors in carbon fluxes ## NA ## Worked on a small compilation for a regional review paper, looking at variation in seagrass soil carbon and sequestration rates. ## digital soil mapping ## Provide broader context for local studies; examine trends across multiple biomes ## NA ## Global change effects on soil biogeochemical cycling, microorganisms, and SOM persistence. (ps. I am more of a data producer, but my students are working on meta-analyses and so I dabble in data compiling). ## What are the relative controls on long term soil carbon storage in different biomes, climates, soil types, with depth, etc. ## USDA-NRCS soil scientist here, it is one of our main objectives ## NA ## I compile data products to look for global scale patterns in ecological or biogeochemical processes. Sometimes the objective is to answer a science question, like are above and belowground phenology in sync? Sometimes it is to come up with parameter values or inputs for models, such as root inputs to rhizosphere soil, or sorption capacity at different sites. ## Soils have been studied for such a long time. Yet, until today it is hard to work with the already existing data since there is no common place were the data get&#39;s stored. So, a lot of people are just compiling data by themselves for their specific question. However, I think it is really important that all the results we have get public available in a way that many people can easily access it. ## In particular, I&#39;m involved in building ISRaD - the International Soil Radiocarbon Database. The idea is to collect 14C data from soils all over the globe and to use the data to better understand global soil C dynamics. ## ## NA ## Usually we seek to ask a big, broad question, more often in the spatial than the temporal domain. ## Compiling data products helps us to get new understanding in exploring scientific questions. I am working on combining big soil data with model to find underlying mechanisms in soil carbon cycle. ## To gain context across scales (soil profile to regional), especially when data is not measured or reported at a given scale. ## Typically to better understand impact of management practices on soil properties ## I am primarily interested in the microclimate of near-surface and sub-surface environments, and what geographical/meteorological/biological factors drive microclimate. ## The primary goal of compiling data in my work is synthesis: trying to leverage the richness of the literature to answer questions about broad scale patterns. ## I have compiled data products to determine the mean responses of soil organic matter pools to global change and to also determine what is driving variation in these responses. I&#39;ve done this using meta-analysis. ## Answer specific questions (e.g. are soil-atmosphere gas fluxes changing with climate change); provide products and platforms for the community. ## I compile data products to help improve decision-support tools for farmers and companies looking to reduce on-farm GHG emissions. My primary research question is how do agriculture best management practices (i.e., cover crops, organic amendments, management-intensive grazing) impact soil C stocks and how does variation in these practices impact SOC response? ## Can we link agricultural management practices with dynamic soil properties? What factors drive heterogeneity in dynamic soil properties at field, landscape, and regional scales? ## crop and ecosystem modeling ## Mostly to initialize spatial watershed management models at the sub-field scale. 11.2.2 How do you find your data sources? ## From the literature ## NA ## Since it was a regional review, and we knew everyone that had worked on this topic, we just reached out to each of them personally. ## public sources of information ## Primarily online databases (e.g., USGS NWIS, USGS Geochemical Landscapes, NEON) but some literature as well when online databases don&#39;t cover the data ## NA ## literature - only because we&#39;re often looking for super specific things (i.e., focus on just the tropics, or in tree soil, etc.) ## Literature searches, my or collaborators original unpublished data, and ISRaD ## sift-through our many databases, collect new information in the field, browse tables in scientific publications / books ## NA ## I have used several methods, depending on the question. One is a standardized literature search, using either google scholar or web of science. Another is pulling data from more curated databases. A third way is contacting groups that I know do a specific kind of experiment and have the data I&#39;m looking for (a targeted approach, I guess). ## Colleagues, ## For articles: scopus, google scholar, contacting authors ## NA ## Web of Science Citation Index, limited keyword search strings ## Published article and contacting data holders for collaboration ## Colleagues, google searches, and scientific publications (usually in that order. Searches lead to multiple options, then verifying quality/accuracy via publications) ## Primary literature, extract from tables &amp; figures ## A combination of publicly available (e.g. PANGAEA), networked (e.g. Ameriflux, AsiaFlux), or privately shared data. Data is either extracted/scraped/rescued by synthesis team, or submitted by primary authors ## Literature review, previous syntheses, crowd-sourced data entry efforts ## I found data sources by searching Web of Science and the ProQuest Agricultural and Environmental Database. In addition I pulled climate data from the WorldClim database. ## Academic search engines; outreach; networking with e.g. program managers. ## Published, peer-reviewed literature, long-term datasets, and collaboration with other researchers that collect these data. ## Peer-reviewed literature search; internet (grey literature) search; asking researchers/colleagues; asking other partners (government agencies, community organizations). ## google, google scholar, colleagues ## Google for those we are not already familiar with 11.2.3 What went well with how you compiled the data product or what do you wish gone better? ## Everything went fairly well (other than how hard and detailed the work is). I wish I had better automated QAQC protocols. ## NA ## It took a while for a few folks to get their data to us. And though we were offering manuscript authorship, we also felt bad knowing they were donating their time when already busy. Would be easier if all data were already in a repository. ## there are always challenges with multiple methods for data collection and discrepancies between methods or further analysis. I would like to collaborate comparing and testing multiple methods for analyzing discrepancies and uncertainties on multiple soil carbon estimates. ## Some platforms have really good interfaces for selectively downloading specific data products from specific sites (e.g., NWIS); others do not (e.g., NEON) and require users to compile and sort the data themselves. Many datasets are also focused on one particular measurement (e.g., C and N) and do not provide contextual data (e.g., bulk chemistry), ## NA ## Wish would be better - More soil data repos!!! ## I appended my data to ISRaD compiled data manually - it was a pain and then I have to redo it whenever there is an ISRaD update or I have new data. But I have had trouble with the ISRaD compile function and haven&#39;t put in the time to troubleshoot and/or write my own script. ## It would be nice if publicly-funded research included an {electronic, tabular, clean} representation available WITH the publication or someplace easy to find. ## NA ## When data is provided to me directly by the data provider (a collaboration with experimenters, say, modeling a specific site), I wish I had communicated better the kind of format that is useful to me, so I didn&#39;t have to do so much curation upon receiving the data. For example, sometimes experimenters are not very clear with metadata, so I have to do some extra communication to make sure I know the units, and the conditions under which some measurements were taken. Everyone has their own shorthand. Also, sometimes experimenters (my past self included) tend to use excel spreadsheets in wide format, with extra columns below or to the side of the main dataset containing averages of other columns, graphs, highlights. Which meant that I had to do a lot of cleaning in excel before I could read the data into my analysis program. ## In general, it takes a long time to develop a spreadshet/template that meets all the criteria to have a smooth and productive data entering process. Our template changed quite often and then one as to go back to all the previous templates to change them accordingly. So, a lot of critical thinking should be done at the beginning to create a good template in order not to change it that often while entering data. ## We were able to motivate a lot of people to help building the database. Yet, it takes also a lot of time to keep track of everything that is going on. So, a good system on keeping track of who is entering when a study and if and when they managed to finish it and if they did not finish it if somebody else can help with it. Especially when a lot of data get&#39;s entered at the same time it get be hard to keep track of everything. This means you should also have an idea where to store the data before it goes online and so on... ## Hackathons were always really helpful to motivate people to work on the database (either coding or data entering) and to get a lot of things done in a short amount of time. ## NA ## In general we work with the data that fits the criteria we are looking for. I suspect there are papers with data that may be useful but if its too hard to extract that data, then we have to skip it. We always cite the papers we do end up using in a supplementary materials or similar. ## The availability of data ## Easy to automate ingestion of data from same source, but extremely time consuming to mix (meta)data formats. ## NA ## The standardization of metadata and vocabulary went well, but the author submission process is sometimes tedious: motivating authors to submit data (let alone curate their data to fit synthesis standards) has proven difficult on several projects. ## The biggest issue faced with compiling data for ISRaD was the lack of adequate documentation in the previous syntheses we relied on to build the foundation of the database. This meant we essentially had to revisit the original sources and start from scratch (although the key benefit was that the sources had already been found). ## The most difficult part of compiling the data product was trying to determine what authors meant when they reported data. The most common issues I had were (1) trying to determine how soils were fractionated, as we had strict definitions for the pools we were considering; (2) what units the soil carbon data was reported in (i.e. they would report g/kg but not whether that was gC/kg soil or gC/ kg fraction); and (3) unclear reporting on the number of replicates, which is needed information for meta-analytical statistics. On the plus side, the vast majority of authors responded to my questions in a timely fashion and there was a clear trend of newer papers providing more of the data in the paper or supplementary. ## Early work was hampered by my inexperience (e.g. in data provenance and reproducibility). Later work was must stronger in terms of these factors and software infrastructure. Diagnostics, quality control, automated reports are all critical capabilities that I didn&#39;t appreciate ten years ago. ## Our search results were comprehensive, but time-consuming to compile. The quality of data reported was a large issue. I also wish we had used a better data extraction tool like Web Plot Digitizer rather than Data Thief. WPD is more user-friendly, more accurate, and easier to share with multiple users. I also think that using Excel to compile this data may not be the most efficient, but Access seemed too time-consuming. It&#39;s hard to find a balance between ease of entry and time to enter/add data. ## I wish that we had started out with a more specific framework or pipeline for organizing, storing, and cleaning the data we received from various sources. We did a bunch of data asks at the beginning of the project and dumped it all in a shared folder... now we are in the stage of sifting through that material, identifying where we need more details or metadata, and starting to think about harmonizing the datasets. Most members of our project time are not familiar with version control systems (git and GitHub) , but this would have been a huge help if we started using it from the beginning. Our project isn&#39;t huge (focused primarily on the state of Minnesota), so some of this can be retrospectively imposed, but since this was our first big data compilation project I don&#39;t think we had a good mental model for what the different steps would be. ## no estimates of uncertainty. I wish there was an API ## More easily integrate the data into the analysis systems, ESRI, Python, and R. 11.2.4 What is the ideal process for data harmonization? For the purposes of this question, this includes gap-filling, unit conversion, as well as standardizing headers and table structure. ## This is a big question and quite context dependent. The only general thing to say is that the ideal process should include transparent but lightweight documentation of the harmonization decisions. ## NA ## NA ## thinking louder, it would be a process that recognizes the data source and return to the source an improved outcome, b) one that follows a protocol considering all soil forming and weathering conditions, and c) that uses levels of data processing organized in versions and each version contains a traceable DOI and supporting code for every process (gap filling, units, header and table structure) also with a traceable DOI. The format should be generic (able to be used in both open source or private software) ## NA ## NA ## response ratios were used to compare across studies ## A simple one that would not break (I have also had trouble with fill functions working in one version and not in a later version, causing us to us to substitute our own for the default ISRaD one - in this case gap-filling missing climate data). ## Start with a standard, bend data to that standard as far as you can without introducing artifacts. I&#39;d suggest adopting standards from the USDA-NRCS / NCSS whenever possible. ## ## Standardization of names and units shouldn&#39;t isn&#39;t difficult, someone has to suggest a template. Units should all be SI. Gap-filling (data imputation) is an entirely different matter and requires serious, deliberate discussion over each affected data element. This applies to re-sampling techniques (genetic horizons -&gt; depth intervals, EA splines, etc.). Table structure is important, but mostly from an organizational perspective: fully normalized schemas are elegant but can be very annoying to work with (many joins for a simple query). There is a balance that has to be found between data archival purposes (fully normalized, no duplication, coded values) ------- data analysis (usually denormalized, lots of duplication, raw values). ## NA ## Oof, I don&#39;t know the answer to this, but I have a lot of opinions. I think if there is gap-filling (for some processes anyway) I would like to know which data are gap-filled and which are original, perhaps with another column as a flag. For units, I prefer SI units, with units clearly labeled in a metadata sheet somewhere that also includes the column name and description of what was measured. For table structure, I like the main data sheet to contain one row of column names, and then data, with any ancillary information in a metadata sheet. I think having lots of headers at the top, especially with merged cells (which I have seen in more than one database) is a headache for me to read in. If data providers are savvy and can use attributes like with netcdf files, that would be fine, especially for global maps. But for other kinds of measurements, an excel or csv file is great as long as it&#39;s in long format with only one row of column names before the data. ## In general, it helps a lot to have controlled vocabulary to make sure that everything is entered in the same way. This makes analysis much easier and also automated gap-filling and unit conversion. ## The headers should be self-explanatory as much as possible and as distinctive as possible in comparison to the other headers. People easily missunderstand headers and enter data at the wrong place. In addition, it seems to be helpful to have some mandatory variables that need to be entered by everyone and additional columns that will only be filled in if the data/information is available. Yet, people tend to try to enter as much as possible (which is good) but sometimes they do not have the data basis and come with somethin up what they think would fit/is right which cannot be checked (e.g. in the original publication). ## NA ## I would love someone to write code to data mine: [a] identify papers of potential interest, [2] comb and extract relevant data, [3] place extracted data into a database. I mean isn&#39;t this what hackers and big tech companies do anyway? Why not do the same for science! ## Standardisation, well structured data table, gap filling ## A set (but reasonably flexible) unified variable list. Deep thought put into a pointed and specific set of goals for the variables will save a lot of time and heartache. Gap filling with expert knowledge where appropriate, or reasonable calculations given sufficient additional measurements, or filling with spatial products. This should of course be done only by a knowledgeable team. We have required unit conversion to be done prior to ingestion and used controlled vocabulary/drop down lists to specify units. By using a rigid QAQC, table structure is set and maintained throughout to help with backward and forward compatibility. ## For meta-analysis, converting to e.g. the standardized mean difference ## Scripted curation, as well as preservation of both original and curated datasets from each submitter, is critical ## This is a big question! With both ISRaD and SIDb we relied on a template-based data entry process in order to smooth the data harmonization process. With SIDb, both the table structure and headers were originally more flexible, but we quickly realized that we could only allow flexibility with one or the other (i.e. structure or controlled vocabulary and rigid headers) or querying the database would be next to impossible. With ISRaD we went pretty far in the opposite direction, i.e. controlling both structure and headers and including a lot of controlled vocabulary. The downside of the ISRaD approach is that it was (and is) a lot of work to maintain. ## My system was to record all the data in its original units and then do a calculation step to transfer all data to the same units. I did this all by hand, so writing a script that could perform these functions for me would be quite helpful. The only hurdle here, though, would be the need of bulk density for many calculations. You could use a pedotranfer function but that could introduce some error. You could avoid this problem by working in concentrations but stock is often preferred to represent absolute amounts. If you are working on differences between treatments, as I was, this isn&#39;t a problem, as stock and concentration provide the same response ratio. It is quite difficult to standardize headers for soil fractions, in particular. My method was to initially record one mineral size fraction number (which may have included adding up a number of silt+clay fractions from an aggregate fractionation) and two particulate fraction numbers (usually a light and heavy fraction, which also may have been a sum of fractions in the paper), with their definitions (i.e. density &gt; 1.8 g/cm3). However, the way the ISRAD database handles fractions may have been better - although with very complex fractionations with aggregate, size, and density fractionations, even that method would be difficult. It may be that combining some data in a standardized way before adding it to a database is the best option. ## Whew, big question. I guess most important is that everything is fully reproducible and reversible–you can always get back to original measured data, warts and all. ## I&#39;m not sure I entirely understand this question, but, in my experience, it is crucial to think through the types of data you want/need in advance and to be more comprehensive initially to avoid the need to revisit data sources at a later point in time. This involves building out all the headers/structure of the data base in advance, and then entering a few preliminary sources of data to test out the system. Then, you can determine ideal unit conversions and gap-filling options. ## We&#39;re just getting into the harmonization stage of our project now. We&#39;ve devoted the most time so far into digging up details about the methods used to collect the various measurements - how was the data collected? Can we find a reference to a standardized method? What are the major permutations (types of extractants, methods of quantification, etc.) of any given method that might be relevant for interpreting the data? ## all of the above, plus provenance tracing and uncertainty quantification / quality assessment / acknowledgement of limits in data and how it should be used ## More easily integrate the data into the analysis systems, ESRI, Python, and R. 11.2.5 Do you want your data to be compiled in a data product? Why or why not? ## Yes! Because what is the point of science, otherwise? ## NA ## Yes, I always contribute data when I can. The more it can be used by others, the better. ## yes because it can be used and revised by others ## Yes, for the same reasons I use such databases. ## Sure. It will increase the visibility and impact of my research ## Always! I want more data out there for people to use. ## Absolutely - I have a lot of unique data and it should be used more widely than in my original research projects! ## It already is, but I&#39;d like these data to be more widely available. Lets talk. ## Yes, I am interested in transdisciplinary synthesis of research products. ## Sure. I haven&#39;t produced my own data for a long time, but I would be happy with those measurements to go into a data product, because I would like them to be useful to a wider range of people. ## Yes. I personally think, once data is published it should also be compiled in a larger data product so that more people can use it and benefit from it. Those products can also be really helpful for modellers to checke their models against measured data. ## NA ## Sure. If it can be helpful to other researchers, then I can only see it as a good thing. ## NA ## Yes. I worry that scientists being too precious with data is slowing progress. ## Yes - amplifies impact of hard-earned data ## Yes, both for use by the core synthesis team but also to be made readily available to the community ## Yes. Open data makes science better! ## Definitely! Field scale data is key for understanding mechanistic processes but to make large scale decisions we need large scale data. ## Most important is to have my data deposited--i.e., not lost. Second most important, yes, in a standardized data product is great for future scientific research and data re-use. Accelerates science. ## Definitely! I want this data to be useful to others doing more comprehensive work. I want to take lessons I&#39;ve learned as a data compiler when I write up manuscripts/publish datasets to make these data useful for others (i.e., relevant level of detail, multiple formats - tables and graphs, etc.). ## Yes - multiple reasons! From a philosophical perspective, I want my scientific work to be cumulative, building on a body of knowledge that was developed by others before me and contributing to something that will continue growing after I&#39;m gone. Also, as a student at a public university whose research is funded by public tax dollars, I feel a duty to make my data available in as many ways as possible (including contributing to data compilations and being available in a repository). In soil science specifically, data compilation is exciting because it allows one to ask questions at a larger scale than is usually logistically / financially feasible for a single study. ## NA ## Not necessarily compiled into a product, but available as a subset of a product that aligns with other like products. I can see issues with multiple contributors information getting combined, while having different methods for collection (P test for example). 11.2.6 What is your main hurdle to contributing to data products and to other people using your data? ## Time ## NA ## Not being sure if data will be credited properly (though I haven&#39;t had any bad experiences so far). ## none,that is how we can boost science advance. but be aware that we need to recognize and engage science as a community in all moments of a scientific effort, e.g. Minasny, B., Fiantis, D., Mulyanto, B., Sulaeman, Y. and Widyatmanti, W.: Global soil science research collaboration in the 21st century: Time to end helicopter research, Geoderma, 373, 114299, doi:10.1016/j.geoderma.2020.114299, 2020. ## ## ‌ ## Finding the appropriate database and formatting the data according to platform specific requirements. ## It takes time to go back and organize the data and get the auxiliary information that other people need. There&#39;s also an underlying fear that some mistakes will be found that I had missed previously. I also would like credit for the data, but it&#39;s not always clear how that credit will be given. I understand co-authorship is not always desirable for the compilers, but if we could think of some other type of incentive beyond just a citation that would be nice. ## (1) experimental data/methods are complex, (2) I haven&#39;t published 75% of the data I generate, (3) don&#39;t know about which repo would be best suited to the data I have, etc. ## Time - difficult in finding/making time to learn a new process and also making up for poorly organizing data in past projects (I always think I did better than I actually did in putting everything in one place) ## Long-term maintenance. Multiple representations / versions of the same data can be a nightmare to manage unless there is an authoritative index. ## It is not always clear what data products, or the form of those products, will be useful to others. ## The time it takes to format and create metadata. Thankfully, I had some help uploading my data to a data archive (the Harvard Forest LTER curates a data archive, and they require everyone who works on their LTER to upload their data to it). At that time, early in my career, it was a substantial amount of work to get everything organized and metadata written. Now, I think I would have been more organized at the start. I think this is where having some kind of standard data format would be useful, as well as educating PhD students and early careers about best practices for managing data and metadata. I would have appreciated knowing more best practices early on! ## If I still plan to write a publication with the data. But once is in an article, it should also go in to a larger data product. ## NA ## Mainly I don&#39;t think people know that we have the data products even though we list it in the paper. ## NA ## If it is published, then it should be available. Otherwise, I am hesitant to include it in a larger study. ## Getting data sharing agreements in place with private landowners, so we can share the data we collect; getting data sharing agreements in place with those we are giving the data to; in some cases, ensuring our data are in the proper formats with the necessary meta-data included. ## Time sunk into curation can be a hurdle, but tends to be low if data is well-documented from the start ## NA ## I have not generated very much data yet, as I am a graduate student. But I imagine the main hurdles would be storing your data in a place and format that is easy to find and understand. ## Overly onerous requirements. I think databases need to make it easier, not harder, to contribute. ## Currently, I would say my main hurdle is knowing which data products to contribute too. It will take some time on my part to figure out where I should contribute my data, which is often something that is easy to put off and forget to do. ## Making it relatively easy for the data contributor to provide data while still upholding a high level of detail for metadata/ documentation ## NA ## Finding a place to contribute it to, though the new USDA PDI program might be trying to accomplish this. 11.2.7 What do you wish was better understood about how your data is collected or should be used? ## Errors vary significantly in the different types of data I have contributed. I wish I could better convey the error of some of these high-variability data types (e.g., fine root biomass). ## NA ## Can&#39;t think of anything. ## the error of measurements ## Typical problem of compiled datasets; it&#39;s difficult to evaluate all data included in the compilation in a way to make sure that collection methods are comparable. I would hope that metadata are included to communiciate some of that information. ## Methods of microbial biomass, CUE, fractionation etc. vary and so do the interpretation and cross-comparability of those results! ## I know how my data is collected, but I think that a lack of standardization for methods limits its use (at the same time, there should not be a one size fits all method for soils - soils do not work that way) ## I suppose it would be nice if everyone using the data understood the effort involved in collecting and making measurements as not everyone gets the field and lab research experience to see that. This might help in understanding the nuances of how measurements of the &quot;same&quot; variable can be measured (see the next question). ## Take a soils class or collaborate with a soil scientist if you hope to get the most out of soil characterization / soil survey data. ## That I can make fairly large changes in how data are collected in analyzed, which may in turn greatly increase their utility to others. However, finding out what others need is a roadblock. ## I appreciate how important, confusing, and difficult to summarize the &quot;Notes&quot; section of an experimenters data sheet it. That is, the huge column to the side of your data sheet where you write all the random stuff that may or may not be important (e.g., &quot;spider found in chamber&quot;, &quot;lots of condensation&quot;, &quot;battery went dead right after reading&quot;, &quot;strong wind during reading&quot;). All that stuff that would not get compiled into a database and cannot be condensed into a column (perhaps you could have a Y/N for the presence of a note, so that we know something may have been up with this measurement). In general, I think the benefit of databases is that if you have enough measurements, the messiness inherent in data collection is averaged out, but it&#39;s worth noting on both sides that field conditions can be very messy. ## NA ## NA ## Dunno ## NA ## Data products with good understanding of the science and methods (or at least with the flexibility to adapt to methods) makes it easier for us to enter it and users to actually access it. ## NA ## The important distinction between primary and secondary authorship, but the critical role of both. Data users should make sure to give credit to both the synthesis authors, as well as the primary authors that collected and submitted their data to the synthesis. ## Of course adequate documentation of the methods and appropriate attribution of credit to authors is important, but methods documentation can be challenging. For example, ISRaD provides a space for defining bulk density methods, but it is hardly ever used, and that can lead to issues with comparing or averaging the wrong data. I think the keys for making peace with sharing data in a synthesis are: 1) fully transparent and reproducible data (where did the data come from, what changes were made), 2) credit to authors, 3) on the authors&#39; end: providing adequate documentation of what you did! ## I wish fractionation schemes were better understood. Dispersed and undispersed soils are sometimes viewed equally, when they actually represent very different functional pools. ## Granular tracking of data use (in analyses) would be great and provide a way for researchers to get &#39;credit&#39; for data that&#39;s subsequently used. ## NA ## NA ## NA ## Our collections are not mature enough to answer this yet. 11.2.8 What is the ideal process for data harmonization from a data provider prospective? For the purposes of this question, this includes gap-filling, unit conversion, standardizing headers, and table structure. ## Similar answer as before - traceability and transparency. ## NA ## NA ## having a version of every change in the original dataset, well documented and explained ## Not sure, but standardized submission is important (and also painful to complete).I don&#39;t think units should be standardized so long as they can be easily identified and readily converted in a database. ## The fact that journals are requiring the data to be archived now helps. It&#39;s easier to compile it in a more widely interpretable fashion when it&#39;s fresh, rather than getting a request three years after publication and having to dig back through old files. I think the harmonization is on the data compilers, not the providers, but I am a provider. What is the incentive to take the time to convert to someone else&#39;s structure, other than contributing to someone else&#39;s project? ## a clear way to explain how the soils were sampled, what methods were used, etc. ## An accurate one - there are so many nuances to many of our measurements including which methods are used that can get lost (e.g. how pH or C content is measured). ## duplicate question? ## In a sense, all of the above. To speak a common language, we need a common vocabulary. ## I think my answer is the same as on the previous page. I definitely used to do all of the things that I gripe about in the data user answers, but now that I&#39;ve seen both sides, I think it helps both data providers and users to keep a clean, machine-readable format, since data providers usually also need to analyse their data. ## NA ## NA ## Well, what you&#39;ve already listed is what I would want....in addition to a smart piece of computer code. ## NA ## Easy, well-documented processes with examples of common hang-ups, plus access to data managers so questions can be responded to reasonably quickly. Gap filling should be well-marked in data products, so there is the implicit trust that real data will be preserved and marked. ## NA ## NA ## There are key parameters for understanding soil data on broader scales that are unfortunately not provided in every source. These include geospatial coordinates (including elevation), climate data (MAT, MAP at minimum), vegetation data (forest, grassland, etc.), and soil taxonomy. Flexible units are also important, e.g. if you provide CO2 fluxes from an incubation, you should provide the data to express the fluxes relative to the amount of carbon in the soil sample or on a mass basis. Of course, every specific type of soil data has special considerations, but the basics (above) should be expected to be provided in every study (note to all of us when reviewing manuscripts!) ## I imagine this would be a little effort as possible. However, if this process could be a bit more collaborative and folks were willing to add their data to a database in a standardized way, rather than having the data compiler guess at the methods, I think that could streamline the process. ## See above. ## I would say providing data in multiple formats - tables and figures. Data compilers may prefer both or one or the other, so it is ideal to provide multiple options. ## NA ## NA ## Simplicity in submitting the data with correct ontology and semantics. "],
["june-comparision-of-data-model-structures.html", "12 June Comparision of data model structures 12.1 Introduction and goals 12.2 Individual PI studies were smaller. 12.3 Vocabulary across studies were not obviously harmonizable. 12.4 Study feature summary 12.5 Inital ontology search for relevant control vocabulary. 12.6 Next steps", " 12 June Comparision of data model structures 12.1 Introduction and goals We considered five data products in this section, selected for their relevance to research driven soil carbon data products: International Soil Carbon Network vs 3 (ISCN), Coastal Carbon Research Coordination Network (CCRCN), International Soil Radiocarbon Database (ISRaD), and two soil warming meta-analyses conducted by Crowther (Crowther et al. (2016)) and vanGestel (van Gestel et al. (2018)). These studies include ongoing studies (CCRCN, ISCN), ongoing with incremental publications (ISRaD:Lawrence et al. (2020)), and completed projects (Crowther:Crowther et al. (2016), vanGestel:van Gestel et al. (2018)). 12.2 Individual PI studies were smaller. In general individual PI projects had smaller data models (see Table 1 and Figure 1). Multi-PI studies tended to have multiple columns describing the same variable. These extra columns describe methods, units, standard deviations, and other quantities related to said variable. While Crowther technically had 8 data tables, most of the data was in three main data tables (Figure 2). In contrast, multi-PI projects had larger data tables with more complex key-ed references across them (Figure 3). This also held true for the number of variables in each study. The single PI studies had between 40 (Crowther: Figure 1) and 56 (vanGestel) unique variable names. Multi-PI studies however had between 144 (CCRCN) and 351 (ISRaD: Figure 2). Table 12.1: Table 1: Increasing the number of researchers involved with a study increased the complexity of the data model. The studies varied in the number of data tables that they each contained, with the single PI study by Crowther only containing 3 tables and the multi-PI study CCRCN containing 12. There was a wider variation in the unique variables in each study, from 28 to 221. These were associated with columns that contained data values, units, and other methods notes. Study ID Table count Variable count Column count Multi PI? Crowther 3 27 33 No vanGestel 6 52 54 No ISCN3 4 108 139 Yes CCRCN 12 124 144 Yes ISRaD 8 216 349 Yes Figure 12.1: Figure 1: Data models with id keys only. Figure 12.2: Figure 2: Crawther data model. An example of a less complex, single PI data model. Figure 12.3: Figure 3: ISRaD data model. An example of a complex, multiPI data model. 12.3 Vocabulary across studies were not obviously harmonizable. Initial efforts to harmonize the vocabulary across studies showed over 580 unique variables out of 924 total variables across all data models. Only 5 variables were commonly shared across all data models. These variables tended to focus on study information, site location, climate, bulk density, organic carbon percentage, sand/silt/clay fractions, pH and cation exchange capasity (see Table 2). Table 12.2: Table 2: Common variables (&gt;2 data models) across data models. Total variable count is 21. variable latitude longitude clay sand silt elevation mean_annual_temperature bulk_density carbon_organic cation_exchange_capacity depth_max depth_min total_carbon mean_annual_precipitation nitrogen_total soil_organic_carbon soil_texture_class vegetation_species carbon_to_nitrogen citation dataset_name drainage_class loss_on_ignition observation_date pH site_name soil_series 13c 14c 15n aspect_degree author caco3 coarse_fraction curator_name curator_organization doi effective_cation_exchange_capacity modification_date reference site slope treatment_duration_start treatment_type aluminum_dithionate aluminum_oxalate aspect_class base_saturation base_sum biome calcium_ extractable contact_name country depth_mean email exchangeable_cations_sum fraction_modern fraction_note fraction_scheme iron_dithionate iron_oxalate layer_name magnesium_extractable pH_h2o potassium_extractable site_description sodium_ extractable soil_horizon soil_type treatment_duration vegetation 2d_position age aluminum_pyrophosphate bulk_density_sample bulk_density_total burn_evidence coarse_size_thresh color contact_email contact_orcid_id datum depth depth_water ecoregion fraction_property horizon land_cover layer_note net_primary_productivity nitrogen_total_stock organic_matter parent_material pH_cacl phosphorus_extractable profile_name silicon_dithionate silicon_oxalate silicon_pyrophosphate slope_shape soil_taxonomy NA 12.4 Study feature summary Common across most models are location (latitude-longitude-elevation), observation time, mean annual temperature, and mean annual precipitation, all of which describe site level characteristics. Depth of core or layer paired with bulk density, organic carbon percentage, sand, silt, clay, pH, soil texture class, cation_exchange_capacity, and 14C describe soil level characteristics. Columns for vegetation class notes were common, but not directly comparable across the studies. Bulk density was typically broken into several categories depending on the measurement method used. 12.4.1 Unique features CCRCN min/max latitude detailed author information ‘one_liner’ summary break out bulk density mass/volume many specific isotopes listed (Am241, C14, Cs137, Be7, Pb210, Ra226) X_class is free text or control vocabulary coastal specific vocabulary inundation/salinity anthropgenic impacts core-level vs site latitude/longitude and elevation ISCN3 disturbance table high level of site details frost free days, ponding, runoff higher then average number of layer-level info fraction table only shared with ISRaD ISRaD interstitial table flux table incubation table fraction table only shared with ISCN3 higher than average number of layer info mineral abundance, mass of element extracted Crowther Author updated data (outside sources) Biome, % Clay, pH, Detailed soil warming data planned temperatures, control temperatures, mean temperatures Cation exchange capasity reported % Nitrogen reported distinguished between total raw carbon and total carbon Difference between detailed_site_id(New Name) and site_id(Old Name) vanGestel mean depth instead of depth of core and layer carbon, nitrogen, and phosphorus pools above and below ground treatments and information about treatments (mean, standard error, size) ‘input’ variable soil horizon and percent soil organic matter 12.5 Inital ontology search for relevant control vocabulary. In general, The Ecosystem Ontology (http://bioportal.bioontology.org/ontologies/ECSO) was the most relevant ontology to this study. However, the control vocabulary was not as method specific as many of the larger data products examined here. We searched http://bioportal.bioontology.org/ for ontologies with four common terms across the studies: ‘soil bulk density’, ‘soil organic carbon’, ‘soil pH’, and ‘soil depth’. None of the data products considered in this study refered to a formalized ontology and instead chose to develop their own or adapt-extend vocabulary from previous data products. We report an inital set of search results for some common terms in the data products considered. The search term ‘soil bulk density’ returned 39 ontology matches (search date: 19 May 2020). Many of these were entries for generalized ‘density’ or ‘bulk density’. The Ecosystem Ontology was the only ontology with a complete match (http://purl.dataone.org/odo/ECSO_00001110), though the definition of this entry was ambiguous. It did not specify if the bulk density was sieved or dry soil, making this challenging to use in a soil study without further specifications. The search term ‘soil organic carbon’ returned 26 ontology matches (search date: 19 May 2020). While many of these referred to soil or carbon independently of soil, two entries were specific to soil organic carbon. Interlinking Ontology for Biological Concepts had a complete match to ‘soil organic carbon’ (http://purl.jp/bio/4/id/200906061124670034), though no units were specified making it ambiguous whether this was a mass fraction or density quantification. The Ecosystem Ontology also had a complete match under ‘organic carbon percentage in soil’ (http://purl.dataone.org/odo/ECSO_00000648), but also had ‘total organic carbon percentage’ (http://purl.dataone.org/odo/ECSO_00002149). While the units in this case were well defined, the method of measurement, similar to bulk density, needed more specificity to make the label broadly applicable. The search term ‘soil pH’ returned 43 ontology matches (search data: 19 May 2020). These hits were dominated by either ‘soil’ or ‘pH’ hits. Only two ontologies had specific soil pH entries. The Ecosystem Ontology had a match for ‘soil pH’ (http://purl.dataone.org/odo/ECSO_00001646) did not specify an extraction method. Interlinking Ontology for Biological Concepts had a match for ‘soil acidity’ (http://purl.jp/bio/4/id/200906080708260606), also without an extraction method specified. The search term ‘soil depth’ returned 33 ontology matches (search date: 19 May 2020). Only two of these hits specifically referred to depth of soil. The Ecosystem Ontology had a match for ‘Soil Depth’ (http://purl.dataone.org/odo/ECSO_00001207), specifically mentioning soil depth in the context of layers. Interlinking Ontology for Biological Concepts had a match for ‘soil depth’ (http://purl.jp/bio/4/id/201006028017141570), which seemed to refer more to the total depth of the soil. 12.6 Next steps All groups in this study have been contacted and confirmed interest in particpating. We are currently expanding the data products being considered to include the Soil Incubation Database, WoSIS, ISCN-2016-template, the LTER soil organic matter working group, and the Soil Heath Database. We have draft the intial questions for the long format interviews below and plan to start conducting interviews in June. By the end of July we expect to have a more general community survey targeted more broadly to the soil science community. We will continue to explore the developed ontologies. 12.6.1 Interview questions Why did you start this study? Describe your workflow for ingesting data sets? What decisions did you make to arrive at this workflow? How would someone get a copy of the data in this study? What would you do differently if you had to start again? What would be the same? References "],
["orginal-proposal.html", "13 Orginal proposal 13.1 Summary 13.2 Background 13.3 Method", " 13 Orginal proposal Name of project: Open source data harmonization: beyond data entry 13.1 Summary In this project, we will develop, implement, and test best practices for compiling transparent, reproducible, harmonized, and extendable data collections for meta-analysis. To do this, we will examine six current meta-analysis efforts in the soil community. Through one-on-one interviews with the PIs and developers as well as a broader community surveys, we will identify the strengths and weaknesses of the approaches used by the individual projects. We will generate a white paper outlining suggested best practices based on our findings. We will then use best practices to prioritize ongoing development of the Soil Organic Carbon Data Rescue and Harmonization (SOC-DRaH), an open community project started by the International Soil Carbon Network (ISCN). The results from this project would provide a solid basis for seeking future funding to benchmark soil carbon dynamics in Earth system models, generate soil maps, and gap fill missing data using machine learning algorithms. 13.2 Background Reanalysis of data from individual projects in a meta-analysis is commonly done by researchers. However, aggregating and preparing data for such an analysis is generally difficult and sometimes unreproducible, because the various data sources are not harmonized. In this context, harmonization means uniform application of a single control vocabularies and data model. In many fields, the main problem is no longer data availability; data can be found in many public online repositories. Nor is the difficulty findability; repositories have expanded their data search capacities through efforts like DataOne, and data accessibility is better now than it has ever been historically. The main problem is harmonizing the various data sources, a problem that urgently needs community guidance. Similar to other research communities, soil scientists have diverse data sets which would benefit from a more global data context. Several efforts to provide this global context by harmonizing soil data have emerged over recent years, a subset of which is presented in Table 1. These meta-analysis efforts reflect a common increase in complexity, as researchers try to build on other meta-analysis (Crowther 2016 to van Gestel 2018 is one example of this) or broaden data aggregation efforts to expand from a single lab effort (Crowther 2016 and van Gestel 2018) to a mult-lab effort (CC-RCN, ISRaD, LTER-SOM, and ISCN3) requiring more active workflow management practices. We also see two aggregation practices emerging, templated vs scripted data ingestion ( See “Method” section). Examining the workflow of these meta-analysis efforts could provide valuable insights into the process and be used to generate best practices recommendations. Table 1. Selected current meta-analysis efforts to harmonize soil data. Single lab or multi-lab refers to whether the compilation team comprised a single PI or multiple PIs. Templated ingest uses a common template; scripted ingest uses unique templates. Citation number references either a published product or a link to ongoing effort. Name Type of soil data Single lab or multi-lab effort Templated or scripted ingest Citation Crowther 2016 | field warming experiments single | template | [1] [1] van Gestel 2018 field warming experiments | si single template | [2] [2] CC-RCN | coastal wetland with focus on high resolution core dating multi script | [3 [3] ISRaD | fraction and radiocarbon isotope multi | tem template [4] LTER-SOM long term ecological studies multi script [5] ISCN3 | regional surveys multi template [6] [1] Crowther, T., Todd-Brown, K., Rowe, C. et al. Quantifying global soil carbon losses in response to warming. Nature 540, 104–108 (2016) doi:10.1038/nature20150. [2] van Gestel, N., Shi, Z., van Groenigen, K. et al. Predicting soil carbon loss with warming. Nature 554, E4–E5 (2018) doi:10.1038/nature25745 [3] Coastal Carbon Research Coordination Network https://serc.si.edu/coastalcarbon [4] Lawrence, C. R., Beem-Miller, J., Hoyt, A. M., et al.: An open-source database for the synthesis of soil radiocarbon data: International Soil Radiocarbon Database (ISRaD) version 1.0, Earth Syst. Sci. Data, 12, 61–76, https://doi.org/10.5194/essd-12-61-2020, 2020. [5] Longterm ecological research soil organic matter group https://github.com/lter/lterwg-som [6] International Soil Carbon Network https://iscn.fluxdata.org/ 13.3 Method We will evaluate each of the soil data harmonization efforts listed in Table 1 and implement the resulting best practices to prioritize ongoing development of SOC-DRaH. Dr Todd-Brown has been involved in a number of these efforts to varying degrees and is uniquely positioned to carry out this research. While the exact outline will change, we expect to find the following trends in these projects. To carry out this evaluation, we will (1) review publications within and beyond the soil community, as well as publicly available code and workflows, (2) contact PIs for one-on-one interviews, and (3) conduct a survey of the soil science community to analyse how other meta-analyses are commonly done and solicit more general feedback on how they should be done. New projects are immediately confronted with the need to develop a control vocabulary and data model. Some groups will expend considerable effort at this stage to include a large diverse group of researchers before aggregating data (ISCN3), while others will extend their data model only as needed (Crowther 2016). We expect there is a clear need in the community for a common control vocabulary and mechanism to regularly revise and update this vocabulary to reflect current methods. Next, projects need an organized workflow to merge and harmonize data sets from different sources into a common data model. Manual entry transcribing the data from source to a common template is expected to be the most common. The templated approach is flexible in the sense that the data source could be encoded in a graph, text from a manuscript, or tabular format and manually transcribed with minimal training. However, once that data is transcribed, revision to the data model often requires manually revisiting the original data source. In addition, the templated approach does not take direct advantage of digitized data tables that are becoming more and more common on online repositories. There are a few projects (LTER-SOM and SOC-DRaH) that instead focus on developing a scripted approach where a piece of code is written uniquely for each data set to convert the data automatically. This scripted approach requires more technical skill but is more robust to changes in the data model, because the original data format is preserved. We expect that, while a scripted approach is more technically robust, it would make it more difficult to recruit community contributions. Finally, projects need to extract insight from these diverse data sets. Larger richer data sets offer more options for analysis. However, the format of the data can also impact usability for final analysis. We expect that many of these studies rely heavily on Excel for data storage and may not be aware of better alternative formats. All code, data products, and white paper generated by this project will be made available via a repository under the ESIP organization GitHub under an MIT or CC-BY license. We anticipate the following deliverables: (1) a general white paper on best practices for data harmonization, supported by (2) survey data and interview transcriptions, and (3) specific recommendations to SOC-DRaH for future feature development to increase usability. Scientifically, we are interested in comparing the control vocabularies and data models developed by each group, specifically, what kind of measurements are common across all projects and is there any consensus on vocabulary. "],
["references.html", "References", " References The following packages were used in this study: (R Core Team 2019, @R-bookdown, @R-datamodelr, @R-distill, @R-dplyr, @R-DT, @R-forcats, @R-ggplot2, @R-knitr, @R-plyr, @R-purrr, @R-readr, @R-rmarkdown, @R-stringr, @R-tibble, @R-tidyr, @R-tidyverse, @bookdown2016, @ggplot22016, @knitr2015, @knitr2014, @plyr2011, @rmarkdown2018, @tidyverse2019) "]
]
