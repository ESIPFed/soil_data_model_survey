Timestamp,Why do you compile data product(s)? What is the question you are looking to answer?,How do you find your data sources?,What went well with how you compiled the data product or what do you wish gone better?,"What is the ideal process for data harmonization? For the purposes of this question, this includes gap-filling, unit conversion, as well as standardizing headers and table structure.",Do you want your data to be compiled in a data product? Why or why not?,What is your main hurdle to contributing to data products and to other people using your data?,What do you wish was better understood about how your data is collected or should be used?,"What is the ideal process for data harmonization from a data provider prospective? For the purposes of this question, this includes gap-filling, unit conversion, standardizing headers, and table structure."
2020-07-08T19:19:45Z,To understand global variability and predictors in carbon fluxes,From the literature,Everything went fairly well (other than how hard and detailed the work is). I wish I had better automated QAQC protocols.,This is a big question and quite context dependent. The only general thing to say is that the ideal process should include transparent but lightweight documentation of the harmonization decisions.,"Yes! Because what is the point of science, otherwise?",Time,"Errors vary significantly in the different types of data I have contributed. I wish I could better convey the error of some of these high-variability data types (e.g., fine root biomass).",Similar answer as before - traceability and transparency.
2020-07-09T11:26:31Z,NA,NA,NA,NA,NA,NA,NA,NA
2020-07-09T11:35:37Z,"Worked on a small compilation for a regional review paper, looking at variation in seagrass soil carbon and sequestration rates.","Since it was a regional review, and we knew everyone that had worked on this topic, we just reached out to each of them personally.","It took a while for a few folks to get their data to us. And though we were offering manuscript authorship, we also felt bad knowing they were donating their time when already busy. Would be easier if all data were already in a repository.",NA,"Yes, I always contribute data when I can. The more it can be used by others, the better.",Not being sure if data will be credited properly (though I haven't had any bad experiences so far).,Can't think of anything.,NA
2020-07-09T11:45:23Z,digital soil mapping,public sources of information,there are always challenges with multiple methods for data collection and discrepancies between methods or further analysis. I would like to collaborate comparing and testing multiple methods for analyzing discrepancies and uncertainties on multiple soil carbon estimates.,"thinking louder, it would be a process that recognizes the data source and return to the source an improved outcome, b) one that follows a protocol considering all soil forming and weathering conditions, and c) that uses levels of data processing organized in versions and each version contains a traceable DOI and supporting code for every process (gap filling, units, header and table structure) also with a traceable DOI. The format should be generic (able to be used in both open source or private software)",yes because it can be used and revised by others,"none,that is how we can boost science advance. but be aware that we need to recognize and engage science as a community in all moments of a scientific effort, e.g. Minasny, B., Fiantis, D., Mulyanto, B., Sulaeman, Y. and Widyatmanti, W.: Global soil science research collaboration in the 21st century: Time to end helicopter research, Geoderma, 373, 114299, doi:10.1016/j.geoderma.2020.114299, 2020.

â€Œ",the error of measurements,"having a version of every change in the original dataset, well documented and explained"
2020-07-09T12:30:05Z,Provide broader context for local studies; examine trends across multiple biomes,"Primarily online databases (e.g., USGS NWIS, USGS Geochemical Landscapes, NEON) but some literature as well when online databases don't cover the data","Some platforms have really good interfaces for selectively downloading specific data products from specific sites (e.g., NWIS); others do not (e.g., NEON) and require users to compile and sort the data themselves. Many datasets are also focused on one particular measurement (e.g., C and N) and do not provide contextual data (e.g., bulk chemistry),",NA,"Yes, for the same reasons I use such databases.",Finding the appropriate database and formatting the data according to platform specific requirements.,Typical problem of compiled datasets; it's difficult to evaluate all data included in the compilation in a way to make sure that collection methods are comparable. I would hope that metadata are included to communiciate some of that information.,"Not sure, but standardized submission is important (and also painful to complete).I don't think units should be standardized so long as they can be easily identified and readily converted in a database."
2020-07-09T13:25:14Z,NA,NA,NA,NA,Sure. It will increase the visibility and impact of my research,"It takes time to go back and organize the data and get the auxiliary information that other people need. There's also an underlying fear that some mistakes will be found that I had missed previously. I also would like credit for the data, but it's not always clear how that credit will be given. I understand co-authorship is not always desirable for the compilers, but if we could think of some other type of incentive beyond just a citation that would be nice.","Methods of microbial biomass, CUE, fractionation etc. vary and so do the interpretation and cross-comparability of those results!","The fact that journals are requiring the data to be archived now helps. It's easier to compile it in a more widely interpretable fashion when it's fresh, rather than getting a request three years after publication and having to dig back through old files. I think the harmonization is on the data compilers, not the providers, but I am a provider. What is the incentive to take the time to convert to someone else's structure, other than contributing to someone else's project?"
2020-07-09T13:50:05Z,"Global change effects on soil biogeochemical cycling, microorganisms, and SOM persistence. (ps. I am more of a data producer, but my students are working on meta-analyses and so I dabble in data compiling).","literature - only because we're often looking for super specific things (i.e., focus on just the tropics, or in tree soil, etc.)",Wish would be better - More soil data repos!!!,response ratios were used to compare across studies,Always! I want more data out there for people to use.,"(1) experimental data/methods are complex, (2) I haven't published 75% of the data I generate, (3) don't know about which repo would be best suited to the data I have, etc.","I know how my data is collected, but I think that a lack of standardization for methods limits its use (at the same time, there should not be a one size fits all method for soils - soils do not work that way)","a clear way to explain how the soils were sampled, what methods were used, etc."
2020-07-09T13:56:59Z,"What are the relative controls on long term soil carbon storage in different biomes, climates, soil types, with depth, etc.","Literature searches, my or collaborators original unpublished data, and ISRaD",I appended my data to ISRaD compiled data manually - it was a pain and then I have to redo it whenever there is an ISRaD update or I have new data. But I have had trouble with the ISRaD compile function and haven't put in the time to troubleshoot and/or write my own script.,"A simple one that would not break (I have also had trouble with fill functions working in one version and not in a later version, causing us to us to substitute our own for the default ISRaD one - in this case gap-filling missing climate data).",Absolutely - I have a lot of unique data and it should be used more widely than in my original research projects!,Time - difficult in finding/making time to learn a new process and also making up for poorly organizing data in past projects (I always think I did better than I actually did in putting everything in one place),"I suppose it would be nice if everyone using the data understood the effort involved in collecting and making measurements as not everyone gets the field and lab research experience to see that. This might help in understanding the nuances of how measurements of the ""same"" variable can be measured (see the next question).",An accurate one - there are so many nuances to many of our measurements including which methods are used that can get lost (e.g. how pH or C content is measured).
2020-07-09T18:13:04Z,"USDA-NRCS soil scientist here, it is one of our main objectives","sift-through our many databases, collect new information in the field, browse tables in scientific publications / books","It would be nice if publicly-funded research included an {electronic, tabular, clean} representation available WITH the publication or someplace easy to find.","Start with a standard, bend data to that standard as far as you can without introducing artifacts. I'd suggest adopting standards from the USDA-NRCS / NCSS whenever possible.

Standardization of names and units shouldn't isn't difficult, someone has to suggest a template. Units should all be SI. Gap-filling (data imputation) is an entirely different matter and requires serious, deliberate discussion over each affected data element. This applies to re-sampling techniques (genetic horizons -> depth intervals, EA splines, etc.). Table structure is important, but mostly from an organizational perspective: fully normalized schemas are elegant but can be very annoying to work with (many joins for a simple query). There is a balance that has to be found between data archival purposes (fully normalized, no duplication, coded values) ------- data analysis (usually denormalized, lots of duplication, raw values).","It already is, but I'd like these data to be more widely available. Lets talk.",Long-term maintenance. Multiple representations / versions of the same data can be a nightmare to manage unless there is an authoritative index.,Take a soils class or collaborate with a soil scientist if you hope to get the most out of soil characterization / soil survey data.,duplicate question?
2020-07-09T19:33:26Z,NA,NA,NA,NA,"Yes, I am interested in transdisciplinary synthesis of research products.","It is not always clear what data products, or the form of those products, will be useful to others.","That I can make fairly large changes in how data are collected in analyzed, which may in turn greatly increase their utility to others.  However, finding out what others need is a roadblock.","In a sense, all of the above.  To speak a common language, we need a common vocabulary."
2020-07-10T07:57:18Z,"I compile data products to look for global scale patterns in ecological or biogeochemical processes. Sometimes the objective is to answer a science question, like are above and belowground phenology in sync? Sometimes it is to come up with parameter values or inputs for models, such as root inputs to rhizosphere soil, or sorption capacity at different sites.","I have used several methods, depending on the question. One is a standardized literature search, using either google scholar or web of science. Another is pulling data from more curated databases. A third way is contacting groups that I know do a specific kind of experiment and have the data I'm looking for (a targeted approach, I guess).","When data is provided to me directly by the data provider (a collaboration with experimenters, say, modeling a specific site), I wish I had communicated better the kind of format that is useful to me, so I didn't have to do so much curation upon receiving the data. For example, sometimes experimenters are not very clear with metadata, so I have to do some extra communication to make sure I know the units, and the conditions under which some measurements were taken. Everyone has their own shorthand. Also, sometimes experimenters (my past self included) tend to use excel spreadsheets in wide format, with extra columns below or to the side of the main dataset containing averages of other columns, graphs, highlights. Which meant that I had to do a lot of cleaning in excel before I could read the data into my analysis program.","Oof, I don't know the answer to this, but I have a lot of opinions. I think if there is gap-filling (for some processes anyway) I would like to know which data are gap-filled and which are original, perhaps with another column as a flag. For units, I prefer SI units, with units clearly labeled in a metadata sheet somewhere that also includes the column name and description of what was measured. For table structure, I like the main data sheet to contain one row of column names, and then data, with any ancillary information in a metadata sheet. I think having lots of headers at the top, especially with merged cells (which I have seen in more than one database) is a headache for me to read in. If data providers are savvy and can use attributes like with netcdf files, that would be fine, especially for global maps. But for other kinds of measurements, an excel or csv file is great as long as it's in long format with only one row of column names before the data.","Sure. I haven't produced my own data for a long time, but I would be happy with those measurements to go into a data product, because I would like them to be useful to a wider range of people.","The time it takes to format and create metadata. Thankfully, I had some help uploading my data to a data archive (the Harvard Forest LTER curates a data archive, and they require everyone who works on their LTER to upload their data to it). At that time, early in my career, it was a substantial amount of work to get everything organized and metadata written. Now, I think I would have been more organized at the start. I think this is where having some kind of standard data format would be useful, as well as educating PhD students and early careers about best practices for managing data and metadata. I would have appreciated knowing more best practices early on!","I appreciate how important, confusing, and difficult to summarize the ""Notes"" section of an experimenters data sheet it. That is, the huge column to the side of your data sheet where you write all the random stuff that may or may not be important (e.g., ""spider found in chamber"", ""lots of condensation"", ""battery went dead right after reading"", ""strong wind during reading""). All that stuff that would not get compiled into a database and cannot be condensed into a column (perhaps you could have a Y/N for the presence of a note, so that we know something may have been up with this measurement). In general, I think the benefit of databases is that if you have enough measurements, the messiness inherent in data collection is averaged out, but it's worth noting on both sides that field conditions can be very messy.","I think my answer is the same as on the previous page. I definitely used to do all of the things that I gripe about in the data user answers, but now that I've seen both sides, I think it helps both data providers and users to keep a clean, machine-readable format, since data providers usually also need to analyse their data."
2020-07-10T11:32:06Z,"Soils have been studied for such a long time. Yet, until today it is hard to work with the already existing data since there is no common place were the data get's stored. So, a lot of people are just compiling data by themselves for their specific question. However, I think it is really important that all the results we have get public available in a way that many people can easily access it. 
In particular, I'm involved in building ISRaD - the International Soil Radiocarbon Database. The idea is to collect 14C data from soils all over the globe and to use the data to better understand global soil C dynamics. 
","Colleagues, 
For articles: scopus, google scholar, contacting authors","In general, it takes a long time to develop a spreadshet/template that meets all the criteria to have a smooth and productive data entering process. Our template changed quite often and then one as to go back to all the previous templates to change them accordingly. So, a lot of critical thinking should be done at the beginning to create a good template in order not to change it that often while entering data.
We were able to motivate a lot of people to help building the database. Yet, it takes also a lot of time to keep track of everything that is going on. So, a good system on keeping track of who is entering when a study and if and when they managed to finish it and if they did not finish it if somebody else can help with it. Especially when a lot of data get's entered at the same time it get be hard to keep track of everything. This means you should also have an idea where to store the data before it goes online and so on...
Hackathons were always really helpful to motivate people to work on the database (either coding or data entering) and to get a lot of things done in a short amount of time.","In general, it helps a lot to have controlled vocabulary to make sure that everything is entered in the same way. This makes analysis much easier and also automated gap-filling and unit conversion. 
The headers should be self-explanatory as much as possible and as distinctive as possible in comparison to the other headers. People easily missunderstand headers and enter data at the wrong place. In addition, it seems to be helpful to have some mandatory variables that need to be entered by everyone and additional columns that will only be filled in if the data/information is available. Yet, people tend to try to enter as much as possible (which is good) but sometimes they do not have the data basis and come with somethin up what they think would fit/is right which cannot be checked (e.g. in the original publication).","Yes. I personally think, once data is published it should also be compiled in a larger data product so that more people can use it and benefit from it. Those products can also be really helpful for modellers to checke their models against measured data.","If I still plan to write a publication with the data. But once is in an article, it should also go in to a larger data product.",NA,NA
2020-07-10T12:34:44Z,NA,NA,NA,NA,NA,NA,NA,NA
2020-07-11T14:44:05Z,"Usually we seek to ask a big, broad question, more often in the spatial than the temporal domain.","Web of Science Citation Index, limited keyword search strings","In general we work with the data that fits the criteria we are looking for. I suspect there are papers with data that may be useful but if its too hard to extract that data, then we have to skip it. We always cite the papers we do end up using in a supplementary materials or similar.","I would love someone to write code to data mine: [a] identify papers of potential interest, [2] comb and extract relevant data, [3] place extracted data into a database. I mean isn't this what hackers and big tech companies do anyway? Why not do the same for science!","Sure. If it can be helpful to other researchers, then I can only see it as a good thing.",Mainly I don't think people know that we have the data products even though we list it in the paper.,Dunno,"Well, what you've already listed is what I would want....in addition to a smart piece of computer code."
2020-07-13T10:50:35Z,Compiling data products helps us to get new understanding in exploring scientific questions. I am working on combining big soil data with model to find underlying mechanisms in soil carbon cycle.,Published article and contacting data holders for collaboration,The availability of data,"Standardisation, well structured data table, gap filling",NA,NA,NA,NA
2020-07-15T16:59:53Z,"To gain context across scales (soil profile to regional), especially when data is not measured or reported at a given scale.","Colleagues, google searches, and scientific publications (usually in that order. Searches lead to multiple options, then verifying quality/accuracy via publications)","Easy to automate ingestion of data from same source, but extremely time consuming to mix (meta)data formats.","A set (but reasonably flexible) unified variable list. Deep thought put into a pointed and specific set of goals for the variables will save a lot of time and heartache. Gap filling with expert knowledge where appropriate, or reasonable calculations given sufficient additional measurements, or filling with spatial products. This should of course be done only by a knowledgeable team. We have required unit conversion to be done prior to ingestion and used controlled vocabulary/drop down lists to specify units. By using a rigid QAQC, table structure is set and maintained throughout to help with backward and forward compatibility.",Yes. I worry that scientists being too precious with data is slowing progress.,"If it is published, then it should be available. Otherwise, I am hesitant to include it in a larger study.",Data products with good understanding of the science and methods (or at least with the flexibility to adapt to methods) makes it easier for us to enter it and users to actually access it.,"Easy, well-documented processes with examples of common hang-ups, plus access to data managers so questions can be responded to reasonably quickly. Gap filling should be well-marked in data products, so there is the implicit trust that real data will be preserved and marked."
2020-07-16T15:54:27Z,Typically to better understand impact of management practices on soil properties,"Primary literature, extract from tables & figures",NA,"For meta-analysis, converting to e.g. the standardized mean difference",Yes - amplifies impact of hard-earned data,"Getting data sharing agreements in place with private landowners, so we can share the data we collect; getting data sharing agreements in place with those we are giving the data to; in some cases, ensuring our data are in the proper formats with the necessary meta-data included.",NA,NA
