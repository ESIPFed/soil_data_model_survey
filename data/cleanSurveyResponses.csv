Timestamp,Why do you compile data product(s)? What is the question you are looking to answer?,How do you find your data sources?,What went well with how you compiled the data product or what do you wish gone better?,"What is the ideal process for data harmonization? For the purposes of this question, this includes gap-filling, unit conversion, as well as standardizing headers and table structure.",Do you want your data to be compiled in a data product? Why or why not?,What is your main hurdle to contributing to data products and to other people using your data?,What do you wish was better understood about how your data is collected or should be used?,"What is the ideal process for data harmonization from a data provider prospective? For the purposes of this question, this includes gap-filling, unit conversion, standardizing headers, and table structure."
2020-07-08T19:19:45Z,To understand global variability and predictors in carbon fluxes,From the literature,Everything went fairly well (other than how hard and detailed the work is). I wish I had better automated QAQC protocols.,This is a big question and quite context dependent. The only general thing to say is that the ideal process should include transparent but lightweight documentation of the harmonization decisions.,"Yes! Because what is the point of science, otherwise?",Time,"Errors vary significantly in the different types of data I have contributed. I wish I could better convey the error of some of these high-variability data types (e.g., fine root biomass).",Similar answer as before - traceability and transparency.
2020-07-09T11:26:31Z,NA,NA,NA,NA,NA,NA,NA,NA
2020-07-09T11:35:37Z,"Worked on a small compilation for a regional review paper, looking at variation in seagrass soil carbon and sequestration rates.","Since it was a regional review, and we knew everyone that had worked on this topic, we just reached out to each of them personally.","It took a while for a few folks to get their data to us. And though we were offering manuscript authorship, we also felt bad knowing they were donating their time when already busy. Would be easier if all data were already in a repository.",NA,"Yes, I always contribute data when I can. The more it can be used by others, the better.",Not being sure if data will be credited properly (though I haven't had any bad experiences so far).,Can't think of anything.,NA
2020-07-09T11:45:23Z,digital soil mapping,public sources of information,there are always challenges with multiple methods for data collection and discrepancies between methods or further analysis. I would like to collaborate comparing and testing multiple methods for analyzing discrepancies and uncertainties on multiple soil carbon estimates.,"thinking louder, it would be a process that recognizes the data source and return to the source an improved outcome, b) one that follows a protocol considering all soil forming and weathering conditions, and c) that uses levels of data processing organized in versions and each version contains a traceable DOI and supporting code for every process (gap filling, units, header and table structure) also with a traceable DOI. The format should be generic (able to be used in both open source or private software)",yes because it can be used and revised by others,"none,that is how we can boost science advance. but be aware that we need to recognize and engage science as a community in all moments of a scientific effort, e.g. Minasny, B., Fiantis, D., Mulyanto, B., Sulaeman, Y. and Widyatmanti, W.: Global soil science research collaboration in the 21st century: Time to end helicopter research, Geoderma, 373, 114299, doi:10.1016/j.geoderma.2020.114299, 2020.

‌",the error of measurements,"having a version of every change in the original dataset, well documented and explained"
2020-07-09T12:30:05Z,Provide broader context for local studies; examine trends across multiple biomes,"Primarily online databases (e.g., USGS NWIS, USGS Geochemical Landscapes, NEON) but some literature as well when online databases don't cover the data","Some platforms have really good interfaces for selectively downloading specific data products from specific sites (e.g., NWIS); others do not (e.g., NEON) and require users to compile and sort the data themselves. Many datasets are also focused on one particular measurement (e.g., C and N) and do not provide contextual data (e.g., bulk chemistry),",NA,"Yes, for the same reasons I use such databases.",Finding the appropriate database and formatting the data according to platform specific requirements.,Typical problem of compiled datasets; it's difficult to evaluate all data included in the compilation in a way to make sure that collection methods are comparable. I would hope that metadata are included to communiciate some of that information.,"Not sure, but standardized submission is important (and also painful to complete).I don't think units should be standardized so long as they can be easily identified and readily converted in a database."
2020-07-09T13:25:14Z,NA,NA,NA,NA,Sure. It will increase the visibility and impact of my research,"It takes time to go back and organize the data and get the auxiliary information that other people need. There's also an underlying fear that some mistakes will be found that I had missed previously. I also would like credit for the data, but it's not always clear how that credit will be given. I understand co-authorship is not always desirable for the compilers, but if we could think of some other type of incentive beyond just a citation that would be nice.","Methods of microbial biomass, CUE, fractionation etc. vary and so do the interpretation and cross-comparability of those results!","The fact that journals are requiring the data to be archived now helps. It's easier to compile it in a more widely interpretable fashion when it's fresh, rather than getting a request three years after publication and having to dig back through old files. I think the harmonization is on the data compilers, not the providers, but I am a provider. What is the incentive to take the time to convert to someone else's structure, other than contributing to someone else's project?"
2020-07-09T13:50:05Z,"Global change effects on soil biogeochemical cycling, microorganisms, and SOM persistence. (ps. I am more of a data producer, but my students are working on meta-analyses and so I dabble in data compiling).","literature - only because we're often looking for super specific things (i.e., focus on just the tropics, or in tree soil, etc.)",Wish would be better - More soil data repos!!!,response ratios were used to compare across studies,Always! I want more data out there for people to use.,"(1) experimental data/methods are complex, (2) I haven't published 75% of the data I generate, (3) don't know about which repo would be best suited to the data I have, etc.","I know how my data is collected, but I think that a lack of standardization for methods limits its use (at the same time, there should not be a one size fits all method for soils - soils do not work that way)","a clear way to explain how the soils were sampled, what methods were used, etc."
2020-07-09T13:56:59Z,"What are the relative controls on long term soil carbon storage in different biomes, climates, soil types, with depth, etc.","Literature searches, my or collaborators original unpublished data, and ISRaD",I appended my data to ISRaD compiled data manually - it was a pain and then I have to redo it whenever there is an ISRaD update or I have new data. But I have had trouble with the ISRaD compile function and haven't put in the time to troubleshoot and/or write my own script.,"A simple one that would not break (I have also had trouble with fill functions working in one version and not in a later version, causing us to us to substitute our own for the default ISRaD one - in this case gap-filling missing climate data).",Absolutely - I have a lot of unique data and it should be used more widely than in my original research projects!,Time - difficult in finding/making time to learn a new process and also making up for poorly organizing data in past projects (I always think I did better than I actually did in putting everything in one place),"I suppose it would be nice if everyone using the data understood the effort involved in collecting and making measurements as not everyone gets the field and lab research experience to see that. This might help in understanding the nuances of how measurements of the ""same"" variable can be measured (see the next question).",An accurate one - there are so many nuances to many of our measurements including which methods are used that can get lost (e.g. how pH or C content is measured).
2020-07-09T18:13:04Z,"USDA-NRCS soil scientist here, it is one of our main objectives","sift-through our many databases, collect new information in the field, browse tables in scientific publications / books","It would be nice if publicly-funded research included an {electronic, tabular, clean} representation available WITH the publication or someplace easy to find.","Start with a standard, bend data to that standard as far as you can without introducing artifacts. I'd suggest adopting standards from the USDA-NRCS / NCSS whenever possible.

Standardization of names and units shouldn't isn't difficult, someone has to suggest a template. Units should all be SI. Gap-filling (data imputation) is an entirely different matter and requires serious, deliberate discussion over each affected data element. This applies to re-sampling techniques (genetic horizons -> depth intervals, EA splines, etc.). Table structure is important, but mostly from an organizational perspective: fully normalized schemas are elegant but can be very annoying to work with (many joins for a simple query). There is a balance that has to be found between data archival purposes (fully normalized, no duplication, coded values) ------- data analysis (usually denormalized, lots of duplication, raw values).","It already is, but I'd like these data to be more widely available. Lets talk.",Long-term maintenance. Multiple representations / versions of the same data can be a nightmare to manage unless there is an authoritative index.,Take a soils class or collaborate with a soil scientist if you hope to get the most out of soil characterization / soil survey data.,duplicate question?
2020-07-09T19:33:26Z,NA,NA,NA,NA,"Yes, I am interested in transdisciplinary synthesis of research products.","It is not always clear what data products, or the form of those products, will be useful to others.","That I can make fairly large changes in how data are collected in analyzed, which may in turn greatly increase their utility to others.  However, finding out what others need is a roadblock.","In a sense, all of the above.  To speak a common language, we need a common vocabulary."
2020-07-10T07:57:18Z,"I compile data products to look for global scale patterns in ecological or biogeochemical processes. Sometimes the objective is to answer a science question, like are above and belowground phenology in sync? Sometimes it is to come up with parameter values or inputs for models, such as root inputs to rhizosphere soil, or sorption capacity at different sites.","I have used several methods, depending on the question. One is a standardized literature search, using either google scholar or web of science. Another is pulling data from more curated databases. A third way is contacting groups that I know do a specific kind of experiment and have the data I'm looking for (a targeted approach, I guess).","When data is provided to me directly by the data provider (a collaboration with experimenters, say, modeling a specific site), I wish I had communicated better the kind of format that is useful to me, so I didn't have to do so much curation upon receiving the data. For example, sometimes experimenters are not very clear with metadata, so I have to do some extra communication to make sure I know the units, and the conditions under which some measurements were taken. Everyone has their own shorthand. Also, sometimes experimenters (my past self included) tend to use excel spreadsheets in wide format, with extra columns below or to the side of the main dataset containing averages of other columns, graphs, highlights. Which meant that I had to do a lot of cleaning in excel before I could read the data into my analysis program.","Oof, I don't know the answer to this, but I have a lot of opinions. I think if there is gap-filling (for some processes anyway) I would like to know which data are gap-filled and which are original, perhaps with another column as a flag. For units, I prefer SI units, with units clearly labeled in a metadata sheet somewhere that also includes the column name and description of what was measured. For table structure, I like the main data sheet to contain one row of column names, and then data, with any ancillary information in a metadata sheet. I think having lots of headers at the top, especially with merged cells (which I have seen in more than one database) is a headache for me to read in. If data providers are savvy and can use attributes like with netcdf files, that would be fine, especially for global maps. But for other kinds of measurements, an excel or csv file is great as long as it's in long format with only one row of column names before the data.","Sure. I haven't produced my own data for a long time, but I would be happy with those measurements to go into a data product, because I would like them to be useful to a wider range of people.","The time it takes to format and create metadata. Thankfully, I had some help uploading my data to a data archive (the Harvard Forest LTER curates a data archive, and they require everyone who works on their LTER to upload their data to it). At that time, early in my career, it was a substantial amount of work to get everything organized and metadata written. Now, I think I would have been more organized at the start. I think this is where having some kind of standard data format would be useful, as well as educating PhD students and early careers about best practices for managing data and metadata. I would have appreciated knowing more best practices early on!","I appreciate how important, confusing, and difficult to summarize the ""Notes"" section of an experimenters data sheet it. That is, the huge column to the side of your data sheet where you write all the random stuff that may or may not be important (e.g., ""spider found in chamber"", ""lots of condensation"", ""battery went dead right after reading"", ""strong wind during reading""). All that stuff that would not get compiled into a database and cannot be condensed into a column (perhaps you could have a Y/N for the presence of a note, so that we know something may have been up with this measurement). In general, I think the benefit of databases is that if you have enough measurements, the messiness inherent in data collection is averaged out, but it's worth noting on both sides that field conditions can be very messy.","I think my answer is the same as on the previous page. I definitely used to do all of the things that I gripe about in the data user answers, but now that I've seen both sides, I think it helps both data providers and users to keep a clean, machine-readable format, since data providers usually also need to analyse their data."
2020-07-10T11:32:06Z,"Soils have been studied for such a long time. Yet, until today it is hard to work with the already existing data since there is no common place were the data get's stored. So, a lot of people are just compiling data by themselves for their specific question. However, I think it is really important that all the results we have get public available in a way that many people can easily access it. 
In particular, I'm involved in building ISRaD - the International Soil Radiocarbon Database. The idea is to collect 14C data from soils all over the globe and to use the data to better understand global soil C dynamics. 
","Colleagues, 
For articles: scopus, google scholar, contacting authors","In general, it takes a long time to develop a spreadshet/template that meets all the criteria to have a smooth and productive data entering process. Our template changed quite often and then one as to go back to all the previous templates to change them accordingly. So, a lot of critical thinking should be done at the beginning to create a good template in order not to change it that often while entering data.
We were able to motivate a lot of people to help building the database. Yet, it takes also a lot of time to keep track of everything that is going on. So, a good system on keeping track of who is entering when a study and if and when they managed to finish it and if they did not finish it if somebody else can help with it. Especially when a lot of data get's entered at the same time it get be hard to keep track of everything. This means you should also have an idea where to store the data before it goes online and so on...
Hackathons were always really helpful to motivate people to work on the database (either coding or data entering) and to get a lot of things done in a short amount of time.","In general, it helps a lot to have controlled vocabulary to make sure that everything is entered in the same way. This makes analysis much easier and also automated gap-filling and unit conversion. 
The headers should be self-explanatory as much as possible and as distinctive as possible in comparison to the other headers. People easily missunderstand headers and enter data at the wrong place. In addition, it seems to be helpful to have some mandatory variables that need to be entered by everyone and additional columns that will only be filled in if the data/information is available. Yet, people tend to try to enter as much as possible (which is good) but sometimes they do not have the data basis and come with somethin up what they think would fit/is right which cannot be checked (e.g. in the original publication).","Yes. I personally think, once data is published it should also be compiled in a larger data product so that more people can use it and benefit from it. Those products can also be really helpful for modellers to checke their models against measured data.","If I still plan to write a publication with the data. But once is in an article, it should also go in to a larger data product.",NA,NA
2020-07-10T12:34:44Z,NA,NA,NA,NA,NA,NA,NA,NA
2020-07-11T14:44:05Z,"Usually we seek to ask a big, broad question, more often in the spatial than the temporal domain.","Web of Science Citation Index, limited keyword search strings","In general we work with the data that fits the criteria we are looking for. I suspect there are papers with data that may be useful but if its too hard to extract that data, then we have to skip it. We always cite the papers we do end up using in a supplementary materials or similar.","I would love someone to write code to data mine: [a] identify papers of potential interest, [2] comb and extract relevant data, [3] place extracted data into a database. I mean isn't this what hackers and big tech companies do anyway? Why not do the same for science!","Sure. If it can be helpful to other researchers, then I can only see it as a good thing.",Mainly I don't think people know that we have the data products even though we list it in the paper.,Dunno,"Well, what you've already listed is what I would want....in addition to a smart piece of computer code."
2020-07-13T10:50:35Z,Compiling data products helps us to get new understanding in exploring scientific questions. I am working on combining big soil data with model to find underlying mechanisms in soil carbon cycle.,Published article and contacting data holders for collaboration,The availability of data,"Standardisation, well structured data table, gap filling",NA,NA,NA,NA
2020-07-15T16:59:53Z,"To gain context across scales (soil profile to regional), especially when data is not measured or reported at a given scale.","Colleagues, google searches, and scientific publications (usually in that order. Searches lead to multiple options, then verifying quality/accuracy via publications)","Easy to automate ingestion of data from same source, but extremely time consuming to mix (meta)data formats.","A set (but reasonably flexible) unified variable list. Deep thought put into a pointed and specific set of goals for the variables will save a lot of time and heartache. Gap filling with expert knowledge where appropriate, or reasonable calculations given sufficient additional measurements, or filling with spatial products. This should of course be done only by a knowledgeable team. We have required unit conversion to be done prior to ingestion and used controlled vocabulary/drop down lists to specify units. By using a rigid QAQC, table structure is set and maintained throughout to help with backward and forward compatibility.",Yes. I worry that scientists being too precious with data is slowing progress.,"If it is published, then it should be available. Otherwise, I am hesitant to include it in a larger study.",Data products with good understanding of the science and methods (or at least with the flexibility to adapt to methods) makes it easier for us to enter it and users to actually access it.,"Easy, well-documented processes with examples of common hang-ups, plus access to data managers so questions can be responded to reasonably quickly. Gap filling should be well-marked in data products, so there is the implicit trust that real data will be preserved and marked."
2020-07-16T15:54:27Z,Typically to better understand impact of management practices on soil properties,"Primary literature, extract from tables & figures",NA,"For meta-analysis, converting to e.g. the standardized mean difference",Yes - amplifies impact of hard-earned data,"Getting data sharing agreements in place with private landowners, so we can share the data we collect; getting data sharing agreements in place with those we are giving the data to; in some cases, ensuring our data are in the proper formats with the necessary meta-data included.",NA,NA
2020-07-20T18:16:04Z,"I am primarily interested in the microclimate of near-surface and sub-surface environments, and what geographical/meteorological/biological factors drive microclimate.","A combination of publicly available (e.g. PANGAEA), networked (e.g. Ameriflux, AsiaFlux), or privately shared data. Data is either extracted/scraped/rescued by synthesis team, or submitted by primary authors","The standardization of metadata and vocabulary went well, but the author submission process is sometimes tedious: motivating authors to submit data (let alone curate their data to fit synthesis standards) has proven difficult on several projects.","Scripted curation, as well as preservation of both original and curated datasets from each submitter, is critical","Yes, both for use by the core synthesis team but also to be made readily available to the community","Time sunk into curation can be a hurdle, but tends to be low if data is well-documented from the start","The important distinction between primary and secondary authorship, but the critical role of both. Data users should make sure to give credit to both the synthesis authors, as well as the primary authors that collected and submitted their data to the synthesis.",NA
2020-07-22T02:38:59Z,The primary goal of compiling data in my work is synthesis: trying to leverage the richness of the literature to answer questions about broad scale patterns.,"Literature review, previous syntheses, crowd-sourced data entry efforts",The biggest issue faced with compiling data for ISRaD was the lack of adequate documentation in the previous syntheses we relied on to build the foundation of the database. This meant we essentially had to revisit the original sources and start from scratch (although the key benefit was that the sources had already been found).,"This is a big question! With both ISRaD and SIDb we relied on a template-based data entry process in order to smooth the data harmonization process. With SIDb, both the table structure and headers were originally more flexible, but we quickly realized that we could only allow flexibility with one or the other (i.e. structure or controlled vocabulary and rigid headers) or querying the database would be next to impossible. With ISRaD we went pretty far in the opposite direction, i.e. controlling both structure and headers and including a lot of controlled vocabulary. The downside of the ISRaD approach is that it was (and is) a lot of work to maintain.",Yes. Open data makes science better!,NA,"Of course adequate documentation of the methods and appropriate attribution of credit to authors is important, but methods documentation can be challenging. For example, ISRaD provides a space for defining bulk density methods, but it is hardly ever used, and that can lead to issues with comparing or averaging the wrong data. I think the keys for making peace with sharing data in a synthesis are: 1) fully transparent and reproducible data (where did the data come from, what changes were made), 2) credit to authors, 3) on the authors' end: providing adequate documentation of what you did!","There are key parameters for understanding soil data on broader scales that are unfortunately not provided in every source. These include geospatial coordinates (including elevation), climate data (MAT, MAP at minimum), vegetation data (forest, grassland, etc.), and soil taxonomy. Flexible units are also important, e.g. if you provide CO2 fluxes from an incubation, you should provide the data to express the fluxes relative to the amount of carbon in the soil sample or on a mass basis. Of course, every specific type of soil data has special considerations, but the basics (above) should be expected to be provided in every study (note to all of us when reviewing manuscripts!)"
2020-07-22T11:04:02Z,I have compiled data products to determine the mean responses of soil organic matter pools to global change and to also determine what is driving variation in these responses. I've done this using meta-analysis.,I found data sources by searching Web of Science and the ProQuest Agricultural and Environmental Database. In addition I pulled climate data from the WorldClim database.,"The most difficult part of compiling the data product was trying to determine what authors meant when they reported data. The most common issues I had were (1) trying to determine how soils were fractionated, as we had strict definitions for the pools we were considering; (2) what units the soil carbon data was reported in (i.e. they would report g/kg but not whether that was gC/kg soil or gC/ kg fraction); and (3) unclear reporting on the number of replicates, which is needed information for meta-analytical statistics. On the plus side, the vast majority of authors responded to my questions in a timely fashion and there was a clear trend of newer papers providing more of the data in the paper or supplementary.","My system was to record all the data in its original units and then do a calculation step to transfer all data to the same units. I did this all by hand, so writing a script that could perform these functions for me would be quite helpful. The only hurdle here, though, would be the need of bulk density for many calculations. You could use a pedotranfer function but that could introduce some error. You could avoid this problem by working in concentrations but stock is often preferred to represent absolute amounts. If you are working on differences between treatments, as I was, this isn't a problem, as stock and concentration provide the same response ratio. It is quite difficult to standardize headers for soil fractions, in particular. My method was to initially record one mineral size fraction number (which may have included adding up a number of silt+clay fractions from an aggregate fractionation) and two particulate fraction numbers (usually a light and heavy fraction, which also may have been a sum of fractions in the paper), with their definitions (i.e. density > 1.8 g/cm3). However, the way the ISRAD database handles fractions may have been better - although with very complex fractionations with aggregate, size, and density fractionations, even that method would be difficult. It may be that combining some data in a standardized way before adding it to a database is the best option.",Definitely! Field scale data is key for understanding mechanistic processes but to make large scale decisions we need large scale data.,"I have not generated very much data yet, as I am a graduate student. But I imagine the main hurdles would be storing your data in a place and format that is easy to find and understand.","I wish fractionation schemes were better understood. Dispersed and undispersed soils are sometimes viewed equally, when they actually represent very different functional pools.","I imagine this would be a little effort as possible. However, if this process could be a bit more collaborative and folks were willing to add their data to a database in a standardized way, rather than  having the data compiler guess at the methods, I think that could streamline the process."
2020-07-23T07:51:09Z,Answer specific questions (e.g. are soil-atmosphere gas fluxes changing with climate change); provide products and platforms for the community.,Academic search engines; outreach; networking with e.g. program managers.,"Early work was hampered by my inexperience (e.g. in data provenance and reproducibility). Later work was must stronger in terms of these factors and software infrastructure. Diagnostics, quality control, automated reports are all critical capabilities that I didn't appreciate ten years ago.","Whew, big question. I guess most important is that everything is fully reproducible and reversible–you can always get back to original measured data, warts and all.","Most important is to have my data deposited--i.e., not lost. Second most important, yes, in a standardized data product is great for future scientific research and data re-use. Accelerates science.","Overly onerous requirements. I think databases need to make it easier, not harder, to contribute.",Granular tracking of data use (in analyses) would be great and provide a way for researchers to get 'credit' for data that's subsequently used.,See above.
2020-07-23T12:48:24Z,"I compile data products to help improve decision-support tools for farmers and companies looking to reduce on-farm GHG emissions. My primary research question is how do agriculture best management practices (i.e., cover crops, organic amendments, management-intensive grazing) impact soil C stocks and how does variation in these practices impact SOC response?","Published, peer-reviewed literature, long-term datasets, and collaboration with other researchers that collect these data.","Our search results were comprehensive, but time-consuming to compile. The quality of data reported was a large issue. I also wish we had used a better data extraction tool like Web Plot Digitizer rather than Data Thief. WPD is more user-friendly, more accurate, and easier to share with multiple users. I also think that using Excel to compile this data may not be the most efficient, but Access seemed too time-consuming. It's hard to find a balance between ease of entry and time to enter/add data.","I'm not sure I entirely understand this question, but, in my experience, it is crucial to think through the types of data you want/need in advance and to be more comprehensive initially to avoid the need to revisit data sources at a later point in time. This involves building out all the headers/structure of the data base in advance, and then entering a few preliminary sources of data to test out the system. Then, you can determine ideal unit conversions and gap-filling options.","Definitely! I want this data to be useful to others doing more comprehensive work. I want to take lessons I've learned as a data compiler when I write up manuscripts/publish datasets to make these data useful for others (i.e., relevant level of detail, multiple formats - tables and graphs, etc.).","Currently, I would say my main hurdle is knowing which data products to contribute too. It will take some time on my part to figure out where I should contribute my data, which is often something that is easy to put off and forget to do.",NA,"I would say providing data in multiple formats - tables and figures. Data compilers may prefer both or one or the other, so it is ideal to provide multiple options."
2020-07-23T13:10:28Z,"Can we link agricultural management practices with dynamic soil properties?  What factors drive heterogeneity in dynamic soil properties at field, landscape, and regional scales?","Peer-reviewed literature search; internet (grey literature) search;  asking researchers/colleagues; asking other partners (government agencies, community organizations).","I wish that we had started out with a more specific framework or pipeline for organizing, storing, and cleaning the data we received from various sources.  We did a bunch of data asks at the beginning of the project and dumped it all in a shared folder... now we are in the stage of sifting through that material, identifying where we need more details or metadata, and starting to think about harmonizing the datasets.   Most members of our project time are not familiar with version control systems (git and GitHub) , but this would have been a huge help if we started using it from the beginning.  Our project isn't huge (focused primarily on the state of  Minnesota), so some of this can be retrospectively imposed, but since this was our first big data compilation project I don't think we had a good mental model for what the different steps would be.","We're just getting into the harmonization stage of our project now.  We've devoted the most time so far into digging up details about the methods used to collect the various measurements - how was the data collected? Can we find a reference to a standardized method? What are the major permutations (types of extractants, methods of quantification, etc.) of any given method that might be relevant for interpreting the data?","Yes - multiple reasons!  From a philosophical perspective, I want my scientific work to be cumulative, building on a body of knowledge that was developed by others before me and contributing to something that will continue growing after I'm gone.  Also, as a student at a public university whose research is funded by public tax dollars, I feel a duty to make my data available in as many ways as possible (including contributing to data compilations and being available in a repository).  In soil science specifically, data compilation is exciting because it allows one to ask questions at a larger scale than is usually logistically / financially feasible for a single study.",Making it relatively easy for the data contributor to provide data while still upholding a high level of detail for metadata/ documentation,NA,NA
2020-07-24T13:45:26Z,crop and ecosystem modeling,"google, google scholar, colleagues",no estimates of uncertainty. I wish there was an API,"all of the above, plus provenance tracing and uncertainty quantification / quality assessment / acknowledgement of limits in data and how it should be used",NA,NA,NA,NA
2020-07-24T15:49:58Z,Mostly to initialize spatial watershed management models at the sub-field scale.,Google for those we are not already familiar with,"More easily integrate the data into the analysis systems, ESRI, Python, and R.","More easily integrate the data into the analysis systems, ESRI, Python, and R.","Not necessarily compiled into a product, but available as a subset of a product that aligns with other like products. I can see issues with multiple contributors information getting combined, while having different methods for collection (P test for example).","Finding a place to contribute it to, though the new USDA PDI program might be trying to accomplish this.",Our collections are not mature enough to answer this yet.,Simplicity in submitting the data with correct ontology and semantics.
